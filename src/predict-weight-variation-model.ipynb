{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7217d519-992c-4b2b-bc84-c101f8f09e0f",
   "metadata": {},
   "source": [
    "## Barcamp2024 - Modelo para predecir variacion de peso corporal usando Deep Learning.\n",
    "\n",
    "Este es un modelo de ejemplo que se desarrollara en este notebook para luego desplegarlo a un ambiente de produccion usando ONNX. Haremos 2 versiones de un mismo modelo: Una en Tensorflow (Google) y otra en PyTorch (Meta) y veremos como usando un mismo runtime con ONNX podremos ejecutarlo independientemente de la libreria donde lo hayamos elaborado.\n",
    "\n",
    "URL del dataset: https://www.kaggle.com/datasets/abdullah0a/comprehensive-weight-change-prediction\n",
    "\n",
    "### Descripcion de las variables del dataset:\n",
    "\n",
    "- **participant_id:** Identificador unico de cada participante del estudio\n",
    "- **age:** La edad del participante (en años), el cual puede influenciar el metabolismo y el cambio de peso.\n",
    "- **gender:** Genero del participante (M/F), ya que las diferencias fisiologicas podrian afectar el manejo del peso.\n",
    "- **current_weight (lbs):** El peso del participante al inicio del estudio, sirviendo como base de comparacion para la variacion de peso.\n",
    "- **bmr (Calories):** Tasa Metabolica Basal, calculado usando la ecuacion de Mifflin-St Jeor, representando el numero de calorias quemadas en reposo.\n",
    "- **daily_calories_consumed:** consumo calorico total por dia, incluyendo la variabilidad para reflejar habitos alimenticios del mundo real.\n",
    "- **daily_caloric_surplus_deficit:** La diferencia entre calorias consumidas y el BMR, indicando si el participante esta en un superavit calorico o en un deficit calorico.\n",
    "- **weight_change (lbs):** El cambio de peso estimado sobre una duracion especificada, basada en un superavit o deficit calorico.\n",
    "- **duration (weeks):** El periodo de tiempo sobre el cual se mide el cambio de peso, que varia entre 1 a 12 semanas.\n",
    "- **physical_activity_level:** Nivel de actividad fisica auto-reportado, categorizado como Sedentario (Sedentary), Ligeramente Activo (Lightly Active), Moderadamente Activo (Moderately Active), o Muy Activo (Very Active.\n",
    "- **sleep_quality:** Calidad del sueño auto-reportada, categorizada como Pobre (Poor), Regular (Fair), Buena (Good), o Excelente (Excellent), el cual puede afectar el manejo del peso.\n",
    "- **stress_level:** Un puntaje numerico (1-10) indicando el nivel de stress percibido por el participante, ya que el stress puede influenciar habitos alimenticios y el peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46b90ce-b8df-4934-87dd-8325073d41ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>current_weight</th>\n",
       "      <th>bmr</th>\n",
       "      <th>daily_calories_consumed</th>\n",
       "      <th>daily_caloric_surplus_deficit</th>\n",
       "      <th>weight_change</th>\n",
       "      <th>duration</th>\n",
       "      <th>physical_activity_level</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>stress_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>228.4</td>\n",
       "      <td>3102.3</td>\n",
       "      <td>3916.0</td>\n",
       "      <td>813.7</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sedentary</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>165.4</td>\n",
       "      <td>2275.5</td>\n",
       "      <td>3823.0</td>\n",
       "      <td>1547.5</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>6</td>\n",
       "      <td>Very Active</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>142.8</td>\n",
       "      <td>2119.4</td>\n",
       "      <td>2785.4</td>\n",
       "      <td>666.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>7</td>\n",
       "      <td>Sedentary</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>145.5</td>\n",
       "      <td>2181.3</td>\n",
       "      <td>2587.3</td>\n",
       "      <td>406.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8</td>\n",
       "      <td>Sedentary</td>\n",
       "      <td>Fair</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>M</td>\n",
       "      <td>155.5</td>\n",
       "      <td>2463.8</td>\n",
       "      <td>3312.8</td>\n",
       "      <td>849.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>Lightly Active</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>152.9</td>\n",
       "      <td>2100.6</td>\n",
       "      <td>2262.4</td>\n",
       "      <td>161.9</td>\n",
       "      <td>-12.513498</td>\n",
       "      <td>9</td>\n",
       "      <td>Sedentary</td>\n",
       "      <td>Poor</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36</td>\n",
       "      <td>M</td>\n",
       "      <td>107.3</td>\n",
       "      <td>1991.3</td>\n",
       "      <td>2933.4</td>\n",
       "      <td>942.1</td>\n",
       "      <td>-2.437307</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderately Active</td>\n",
       "      <td>Poor</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>207.9</td>\n",
       "      <td>2977.9</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1022.1</td>\n",
       "      <td>-35.678115</td>\n",
       "      <td>11</td>\n",
       "      <td>Moderately Active</td>\n",
       "      <td>Poor</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>169.5</td>\n",
       "      <td>2406.9</td>\n",
       "      <td>3890.2</td>\n",
       "      <td>1483.4</td>\n",
       "      <td>-8.476633</td>\n",
       "      <td>10</td>\n",
       "      <td>Very Active</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>169.2</td>\n",
       "      <td>2403.3</td>\n",
       "      <td>2485.8</td>\n",
       "      <td>82.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Sedentary</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age gender  current_weight     bmr  daily_calories_consumed  \\\n",
       "0   56      M           228.4  3102.3                   3916.0   \n",
       "1   46      F           165.4  2275.5                   3823.0   \n",
       "2   32      F           142.8  2119.4                   2785.4   \n",
       "3   25      F           145.5  2181.3                   2587.3   \n",
       "4   38      M           155.5  2463.8                   3312.8   \n",
       "5   56      F           152.9  2100.6                   2262.4   \n",
       "6   36      M           107.3  1991.3                   2933.4   \n",
       "7   40      M           207.9  2977.9                   4000.0   \n",
       "8   28      F           169.5  2406.9                   3890.2   \n",
       "9   28      F           169.2  2403.3                   2485.8   \n",
       "\n",
       "   daily_caloric_surplus_deficit  weight_change  duration  \\\n",
       "0                          813.7       0.200000         1   \n",
       "1                         1547.5       2.400000         6   \n",
       "2                          666.0       1.400000         7   \n",
       "3                          406.0       0.800000         8   \n",
       "4                          849.0       2.000000        10   \n",
       "5                          161.9     -12.513498         9   \n",
       "6                          942.1      -2.437307         2   \n",
       "7                         1022.1     -35.678115        11   \n",
       "8                         1483.4      -8.476633        10   \n",
       "9                           82.5       0.000000         2   \n",
       "\n",
       "  physical_activity_level sleep_quality  stress_level  \n",
       "0               Sedentary     Excellent             6  \n",
       "1             Very Active     Excellent             6  \n",
       "2               Sedentary          Good             3  \n",
       "3               Sedentary          Fair             2  \n",
       "4          Lightly Active          Good             1  \n",
       "5               Sedentary          Poor             6  \n",
       "6       Moderately Active          Poor             5  \n",
       "7       Moderately Active          Poor             9  \n",
       "8             Very Active          Poor             1  \n",
       "9               Sedentary     Excellent             7  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# Config adicionales para pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Importando dataset.\n",
    "dataset_path = '../Dataset/weight_change_dataset.csv'\n",
    "\n",
    "dataset_data = pd.read_csv(dataset_path)\n",
    "dataset_data = dataset_data.drop(['participant_id'], axis=1)\n",
    "dataset_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b5756-9bdf-4e1f-ad14-7cda96804d68",
   "metadata": {},
   "source": [
    "## Describiendo los datos\n",
    "\n",
    "Observando las propiedades estadisticas de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9824cd-8e7c-4257-82c9-7e55f6733784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>current_weight</th>\n",
       "      <th>bmr</th>\n",
       "      <th>daily_calories_consumed</th>\n",
       "      <th>daily_caloric_surplus_deficit</th>\n",
       "      <th>weight_change</th>\n",
       "      <th>duration</th>\n",
       "      <th>stress_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.910000</td>\n",
       "      <td>171.533000</td>\n",
       "      <td>2518.206000</td>\n",
       "      <td>3518.292000</td>\n",
       "      <td>1000.091000</td>\n",
       "      <td>-2.779817</td>\n",
       "      <td>6.920000</td>\n",
       "      <td>4.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.219454</td>\n",
       "      <td>30.333856</td>\n",
       "      <td>364.431221</td>\n",
       "      <td>513.313097</td>\n",
       "      <td>371.560827</td>\n",
       "      <td>7.443719</td>\n",
       "      <td>3.515277</td>\n",
       "      <td>2.576879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1566.500000</td>\n",
       "      <td>2030.900000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>-35.678115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>153.650000</td>\n",
       "      <td>2255.050000</td>\n",
       "      <td>3233.300000</td>\n",
       "      <td>766.950000</td>\n",
       "      <td>-5.012312</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>172.150000</td>\n",
       "      <td>2519.500000</td>\n",
       "      <td>3636.050000</td>\n",
       "      <td>1013.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.250000</td>\n",
       "      <td>192.475000</td>\n",
       "      <td>2805.975000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>1253.325000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>238.200000</td>\n",
       "      <td>3390.800000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>1922.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  current_weight          bmr  daily_calories_consumed  \\\n",
       "count  100.000000      100.000000   100.000000               100.000000   \n",
       "mean    37.910000      171.533000  2518.206000              3518.292000   \n",
       "std     12.219454       30.333856   364.431221               513.313097   \n",
       "min     18.000000      100.000000  1566.500000              2030.900000   \n",
       "25%     26.750000      153.650000  2255.050000              3233.300000   \n",
       "50%     38.000000      172.150000  2519.500000              3636.050000   \n",
       "75%     46.250000      192.475000  2805.975000              4000.000000   \n",
       "max     59.000000      238.200000  3390.800000              4000.000000   \n",
       "\n",
       "       daily_caloric_surplus_deficit  weight_change    duration  stress_level  \n",
       "count                     100.000000     100.000000  100.000000    100.000000  \n",
       "mean                     1000.091000      -2.779817    6.920000      4.810000  \n",
       "std                       371.560827       7.443719    3.515277      2.576879  \n",
       "min                        82.500000     -35.678115    1.000000      1.000000  \n",
       "25%                       766.950000      -5.012312    4.000000      2.750000  \n",
       "50%                      1013.100000       0.100000    7.000000      5.000000  \n",
       "75%                      1253.325000       1.850000   10.000000      7.000000  \n",
       "max                      1922.500000       5.000000   12.000000      9.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describiendo el dataset en la parte numerica.\n",
    "dataset_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6b05c1-1ab2-475e-9105-01ecccb1c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcionalidad para verificar la distribucion de los datos.\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def skewed_distribution(data, features, title):\n",
    "    figure = pl.figure(figsize = (11,5))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = figure.add_subplot(1, 4, i+1)\n",
    "        ax.hist(data[feature], bins = 25, color = '#00A0A0')\n",
    "        ax.set_title(\"'%s'\"%(feature), fontsize = 14)\n",
    "        ax.set_xlabel(\"Valor\")\n",
    "        ax.set_ylabel(\"Numero de registros\")\n",
    "        ax.set_ylim((0, 140))\n",
    "        ax.set_yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140])\n",
    "        ax.set_yticklabels([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, \">140\"])\n",
    "        figure.suptitle(title, fontsize = 16, y = 1.03)\n",
    "        figure.tight_layout()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f064bc-c2a8-47bf-8c7c-ef7ed4ecb02a",
   "metadata": {},
   "source": [
    "## Distribucion de los features\n",
    "\n",
    "Aqui veremos la distribucion de los features con la desviacion estandar mas alta para saber que tan sesgados estan y asi realizar ajustes de los mismos durante el feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db30031f-c3d1-4288-8214-54359069d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_to_check = ['bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'stress_level']\n",
    "plot_title = \"Distribucion de los features altamente sesgados\"\n",
    "skewed_distribution(dataset_data, features_to_check, plot_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a57e80-77ce-4de0-8a5e-f0455d29d1f5",
   "metadata": {},
   "source": [
    "Por lo visto **daily_calories_consumed** es un feature que esta altamente sesgado a la izquierda y por tanto habrá que transformarlo para que se comporte como una distribucion normal (Gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e8a1c-1ff3-4172-9bef-0458a6dd6dfc",
   "metadata": {},
   "source": [
    "## Feature engineering.\n",
    "\n",
    "En este paso generaremos los features que el modelo usara para poder entrenarse. Esto incluye:\n",
    "\n",
    "- Separacion de los features de la variable objetivo (la variable que queremos predecir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14acd70-aaef-48f7-9925-94630611a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separacion de la variable objetivo del resto de features.\n",
    "variacion_peso = dataset_data['weight_change']\n",
    "\n",
    "# Separacion de los features de la variable objetivo.\n",
    "features_data = dataset_data.drop('weight_change', axis = 1)\n",
    "pred_pipeline_data = features_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f46c1-d662-4812-bf33-444857b83db9",
   "metadata": {},
   "source": [
    "## Pre-procesamiento.\n",
    "\n",
    "En este paso vamos a pre-procesar los features para que el modelo/algoritmo de deep learning pueda entrenarse de la manera correcta y pueda producir predicciones de la forma mas integra posible. Esto incluye:\n",
    "\n",
    "- Transformacion de los features numericos altamente sesgados.\n",
    "- Normalizacion de los features numericos.\n",
    "- Transformacion de los features categoricos a features numericos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca225970-dddf-4316-9ad9-a32c01ba5c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_sesgados = ['daily_calories_consumed']\n",
    "features_data[features_sesgados] = dataset_data[features_sesgados].apply(lambda x: np.log(x + 1))\n",
    "skewed_distribution(features_data, features_to_check, plot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd47a0c5-945b-4946-aadf-6827e2b156d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>current_weight</th>\n",
       "      <th>bmr</th>\n",
       "      <th>daily_calories_consumed</th>\n",
       "      <th>daily_caloric_surplus_deficit</th>\n",
       "      <th>duration</th>\n",
       "      <th>physical_activity_level</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>stress_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.926829</td>\n",
       "      <td>M</td>\n",
       "      <td>0.929088</td>\n",
       "      <td>0.841857</td>\n",
       "      <td>0.957341</td>\n",
       "      <td>0.397391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sedentary</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age gender  current_weight       bmr  daily_calories_consumed  \\\n",
       "0  0.926829      M        0.929088  0.841857                 0.957341   \n",
       "\n",
       "   daily_caloric_surplus_deficit  duration physical_activity_level  \\\n",
       "0                       0.397391       0.0               Sedentary   \n",
       "\n",
       "  sleep_quality  stress_level  \n",
       "0     Excellent         0.625  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizacion de los features numericos.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_numericos = ['age', 'current_weight', 'bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'duration', 'stress_level']\n",
    "features_data[features_numericos] = scaler.fit_transform(dataset_data[features_numericos])\n",
    "features_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39889b79-9f9b-40de-a8e0-5938d7d5d89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>current_weight</th>\n",
       "      <th>bmr</th>\n",
       "      <th>daily_calories_consumed</th>\n",
       "      <th>daily_caloric_surplus_deficit</th>\n",
       "      <th>duration</th>\n",
       "      <th>stress_level</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>physical_activity_level_Lightly Active</th>\n",
       "      <th>physical_activity_level_Moderately Active</th>\n",
       "      <th>physical_activity_level_Sedentary</th>\n",
       "      <th>physical_activity_level_Very Active</th>\n",
       "      <th>sleep_quality_Excellent</th>\n",
       "      <th>sleep_quality_Fair</th>\n",
       "      <th>sleep_quality_Good</th>\n",
       "      <th>sleep_quality_Poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.929088</td>\n",
       "      <td>0.841857</td>\n",
       "      <td>0.957341</td>\n",
       "      <td>0.397391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.473227</td>\n",
       "      <td>0.388642</td>\n",
       "      <td>0.910111</td>\n",
       "      <td>0.796196</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.309696</td>\n",
       "      <td>0.303075</td>\n",
       "      <td>0.383170</td>\n",
       "      <td>0.317120</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.329233</td>\n",
       "      <td>0.337006</td>\n",
       "      <td>0.282566</td>\n",
       "      <td>0.175815</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.401592</td>\n",
       "      <td>0.491860</td>\n",
       "      <td>0.651008</td>\n",
       "      <td>0.416576</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>0.579017</td>\n",
       "      <td>0.825402</td>\n",
       "      <td>0.516793</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.119392</td>\n",
       "      <td>0.093186</td>\n",
       "      <td>0.368493</td>\n",
       "      <td>0.509511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.453690</td>\n",
       "      <td>0.341008</td>\n",
       "      <td>0.667005</td>\n",
       "      <td>0.583315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.577424</td>\n",
       "      <td>0.605931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.676957</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.616498</td>\n",
       "      <td>0.692978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590652</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  current_weight       bmr  daily_calories_consumed  \\\n",
       "0   0.926829        0.929088  0.841857                 0.957341   \n",
       "1   0.682927        0.473227  0.388642                 0.910111   \n",
       "2   0.341463        0.309696  0.303075                 0.383170   \n",
       "3   0.170732        0.329233  0.337006                 0.282566   \n",
       "4   0.487805        0.401592  0.491860                 0.651008   \n",
       "..       ...             ...       ...                      ...   \n",
       "95  1.000000        0.592619  0.579017                 0.825402   \n",
       "96  0.926829        0.119392  0.093186                 0.368493   \n",
       "97  0.975610        0.453690  0.341008                 0.667005   \n",
       "98  0.658537        0.577424  0.605931                 1.000000   \n",
       "99  0.146341        0.616498  0.692978                 1.000000   \n",
       "\n",
       "    daily_caloric_surplus_deficit  duration  stress_level  gender_F  gender_M  \\\n",
       "0                        0.397391  0.000000         0.625         0         1   \n",
       "1                        0.796196  0.454545         0.625         1         0   \n",
       "2                        0.317120  0.545455         0.250         1         0   \n",
       "3                        0.175815  0.636364         0.125         1         0   \n",
       "4                        0.416576  0.818182         0.000         0         1   \n",
       "..                            ...       ...           ...       ...       ...   \n",
       "95                       0.516793  0.090909         0.250         0         1   \n",
       "96                       0.509511  1.000000         1.000         1         0   \n",
       "97                       0.583315  1.000000         0.750         1         0   \n",
       "98                       0.676957  0.363636         0.500         0         1   \n",
       "99                       0.590652  0.727273         0.750         0         1   \n",
       "\n",
       "    physical_activity_level_Lightly Active  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        1   \n",
       "..                                     ...   \n",
       "95                                       1   \n",
       "96                                       0   \n",
       "97                                       1   \n",
       "98                                       0   \n",
       "99                                       0   \n",
       "\n",
       "    physical_activity_level_Moderately Active  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "..                                        ...   \n",
       "95                                          0   \n",
       "96                                          1   \n",
       "97                                          0   \n",
       "98                                          1   \n",
       "99                                          0   \n",
       "\n",
       "    physical_activity_level_Sedentary  physical_activity_level_Very Active  \\\n",
       "0                                   1                                    0   \n",
       "1                                   0                                    1   \n",
       "2                                   1                                    0   \n",
       "3                                   1                                    0   \n",
       "4                                   0                                    0   \n",
       "..                                ...                                  ...   \n",
       "95                                  0                                    0   \n",
       "96                                  0                                    0   \n",
       "97                                  0                                    0   \n",
       "98                                  0                                    0   \n",
       "99                                  0                                    1   \n",
       "\n",
       "    sleep_quality_Excellent  sleep_quality_Fair  sleep_quality_Good  \\\n",
       "0                         1                   0                   0   \n",
       "1                         1                   0                   0   \n",
       "2                         0                   0                   1   \n",
       "3                         0                   1                   0   \n",
       "4                         0                   0                   1   \n",
       "..                      ...                 ...                 ...   \n",
       "95                        1                   0                   0   \n",
       "96                        0                   0                   1   \n",
       "97                        0                   1                   0   \n",
       "98                        0                   1                   0   \n",
       "99                        0                   1                   0   \n",
       "\n",
       "    sleep_quality_Poor  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "..                 ...  \n",
       "95                   0  \n",
       "96                   0  \n",
       "97                   0  \n",
       "98                   0  \n",
       "99                   0  \n",
       "\n",
       "[100 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformacion de features categoricos a numericos (One-Hot Encoding)\n",
    "\n",
    "features = pd.get_dummies(features_data, dtype='int')\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e8ead-70cc-400b-ad38-fd639842c7bf",
   "metadata": {},
   "source": [
    "## Creando modelo de deep learning.\n",
    "\n",
    "Ya con la data debidamente preparada y pre-procesada, ahora crearemos nuestro modelo de deep learning, una Deep Neural Network (DNN) tradicional para problemas de regresion ya que inicialmente la variable a predecir/estimar es una variable numerica continua (weight_change que es la del cambio de peso).\n",
    "\n",
    "Inicialmente usaremos TensorFlow y luego utilizaremos PyTorch para entrenar el mismo modelo en 2 librerias distintas con fines a ver la diferencia entre el deployment a produccion de forma tradicional con los mecanismos de cada herramienta y luego haciendolo con ONNX.\n",
    "\n",
    "Primero tendremos que crear los training_set, validation_set y test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde7308e-88e1-4086-8d92-9b925d14469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 19:19:07.875662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features train length: 72 \n",
      "Features validation length: 15 \n",
      "Features test length: 13 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from preprocessing_pipelines import RegPredictionDataPrep\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def pre_process_data_point(data_point, highly_skewed_features, numerical_features):\n",
    "    '''\n",
    "    Pre-process adequately a data point we want to predict\n",
    "    Args: data_point (dictionary object with the features we want to make a prediction on)\n",
    "    '''\n",
    "    data_point_df = pd.DataFrame(data_point, index=[0])\n",
    "    data_point_df_copy = data_point_df\n",
    "\n",
    "    # Aplicando logaritmo natural a features altamente sesgados.\n",
    "    data_point_df[highly_skewed_features] = data_point_df[highly_skewed_features].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "    # Escalando los features para que tengan igual tratamiento\n",
    "    feat_scaler = MinMaxScaler()\n",
    "    data_point_df[numerical_features] = feat_scaler.fit_transform(data_point_df_copy[numerical_features])\n",
    "    print(\"Data\")\n",
    "    print(data_point_df)\n",
    "    print(\"END Data\")\n",
    "\n",
    "    # Convirtiendo los features categoricos en features numericos.\n",
    "    data_point_df = pd.get_dummies(data_point_df, dtype='int')\n",
    "    \n",
    "\n",
    "    return data_point_df\n",
    "    \n",
    "\n",
    "# Split dataset into train, validation and tests sets\n",
    "def split_dataset(X_data, Y_data, dataset_size, pct_validation, pct_test):\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=pct_validation)\n",
    "    X_train2, X_test, Y_train2, Y_test = train_test_split(X_train, Y_train, test_size=pct_test)\n",
    "\n",
    "    return X_train2, X_val, X_test, Y_train2, Y_val, Y_test\n",
    "\n",
    "\n",
    "features_train, features_val, features_test, target_train, target_val, target_test = split_dataset(features, variacion_peso, len(features), 0.15, 0.15)\n",
    "\n",
    "print(f\"Features train length: %s \" % len(features_train))\n",
    "print(f\"Features validation length: %s \" % len(features_val))\n",
    "print(f\"Features test length: %s \" % len(features_test))\n",
    "\n",
    "# Converting splitted dataset to tensors for the DNN.\n",
    "\n",
    "t_features_train = tf.convert_to_tensor(features_train)\n",
    "t_features_val = tf.convert_to_tensor(features_val)\n",
    "t_features_test = tf.convert_to_tensor(features_test)\n",
    "\n",
    "t_target_train = tf.convert_to_tensor(target_train)\n",
    "t_target_val = tf.convert_to_tensor(target_val)\n",
    "t_target_test = tf.convert_to_tensor(target_test)\n",
    "\n",
    "# Testing pre-processing function.\n",
    "sample_data_point = {\n",
    "    \"age\": 23,\n",
    "    \"gender\": \"M\",\n",
    "    \"current_weight\": 155,\n",
    "    \"bmr\": 2500.0,\n",
    "    \"daily_calories_consumed\": 3600.0,\n",
    "    \"daily_caloric_surplus_deficit\": 50.0,\n",
    "    \"duration\": 75.0,\n",
    "    \"stress_level\": 70,\n",
    "    \"physical_activity_level\": \"Sedentary\",\n",
    "    \"sleep_quality\": \"Excellent\"\n",
    "}\n",
    "\n",
    "data_point_highly_skewed_features = ['daily_calories_consumed']\n",
    "data_point_numerical_features = ['age', 'current_weight', 'bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'duration', 'stress_level']\n",
    "\n",
    "features_names = list(features.keys())\n",
    "prediction_pipeline = RegPredictionDataPrep(pred_pipeline_data, data_point_highly_skewed_features, data_point_numerical_features, features_names)\n",
    "\n",
    "sample_pre_processed_datapoint = prediction_pipeline.parse_features(sample_data_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76aa185-ff9c-4385-83ee-894c3d26c933",
   "metadata": {},
   "source": [
    "## Creando y entrenando el modelo.\n",
    "\n",
    "Procederemos a crear una DNN (Deep Neural Network) para fines de regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a90f4ae-6f73-4313-b782-fa0dea118218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m270\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> (1.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m390\u001b[0m (1.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> (1.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m390\u001b[0m (1.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creando el modelo\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(17,)))\n",
    "model.add(tf.keras.layers.Dense(15, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(7, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2609e2a-f65e-4aad-8417-08b70d30acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 8.6053 - r2_score: -0.0301\n",
      "Epoch 1: val_loss improved from inf to 104.73937, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 39.4395 - r2_score: -0.1195 - val_loss: 104.7394 - val_r2_score: -0.3628\n",
      "Epoch 2/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 32.3804 - r2_score: -0.0105\n",
      "Epoch 2: val_loss improved from 104.73937 to 102.07235, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 51.4320 - r2_score: -0.1651 - val_loss: 102.0723 - val_r2_score: -0.3281\n",
      "Epoch 3/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 53.7903 - r2_score: -0.2879\n",
      "Epoch 3: val_loss improved from 102.07235 to 99.64245, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 53.5799 - r2_score: -0.1856 - val_loss: 99.6424 - val_r2_score: -0.2964\n",
      "Epoch 4/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 15.8180 - r2_score: 0.0188\n",
      "Epoch 4: val_loss improved from 99.64245 to 97.65160, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 34.9715 - r2_score: -0.0509 - val_loss: 97.6516 - val_r2_score: -0.2705\n",
      "Epoch 5/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 28.5681 - r2_score: -0.0735\n",
      "Epoch 5: val_loss improved from 97.65160 to 95.44624, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.9474 - r2_score: -0.1176 - val_loss: 95.4462 - val_r2_score: -0.2418\n",
      "Epoch 6/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 67.4556 - r2_score: -0.1285\n",
      "Epoch 6: val_loss improved from 95.44624 to 93.72142, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 58.5728 - r2_score: -0.1273 - val_loss: 93.7214 - val_r2_score: -0.2194\n",
      "Epoch 7/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 16.8021 - r2_score: 0.0431\n",
      "Epoch 7: val_loss improved from 93.72142 to 92.32038, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 39.5029 - r2_score: -0.0452 - val_loss: 92.3204 - val_r2_score: -0.2012\n",
      "Epoch 8/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 55.5124 - r2_score: -0.5751\n",
      "Epoch 8: val_loss improved from 92.32038 to 91.13647, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 47.6285 - r2_score: -0.1265 - val_loss: 91.1365 - val_r2_score: -0.1858\n",
      "Epoch 9/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.4917 - r2_score: -0.5042\n",
      "Epoch 9: val_loss improved from 91.13647 to 90.00047, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 45.3251 - r2_score: -0.0900 - val_loss: 90.0005 - val_r2_score: -0.1710\n",
      "Epoch 10/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 14.9062 - r2_score: 0.0652\n",
      "Epoch 10: val_loss improved from 90.00047 to 88.94370, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 37.8585 - r2_score: -0.0157 - val_loss: 88.9437 - val_r2_score: -0.1572\n",
      "Epoch 11/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 101.4418 - r2_score: -0.5241\n",
      "Epoch 11: val_loss improved from 88.94370 to 88.02469, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 54.6195 - r2_score: -0.0816 - val_loss: 88.0247 - val_r2_score: -0.1453\n",
      "Epoch 12/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 38.6322 - r2_score: 0.0236\n",
      "Epoch 12: val_loss improved from 88.02469 to 87.16806, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 37.5937 - r2_score: 0.0200 - val_loss: 87.1681 - val_r2_score: -0.1341\n",
      "Epoch 13/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.0264 - r2_score: 0.0588\n",
      "Epoch 13: val_loss improved from 87.16806 to 86.33898, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 31.5041 - r2_score: 0.0372 - val_loss: 86.3390 - val_r2_score: -0.1234\n",
      "Epoch 14/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 27.8254 - r2_score: -0.3780\n",
      "Epoch 14: val_loss improved from 86.33898 to 85.38881, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 42.2656 - r2_score: -0.0527 - val_loss: 85.3888 - val_r2_score: -0.1110\n",
      "Epoch 15/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.9970 - r2_score: -1.2793\n",
      "Epoch 15: val_loss improved from 85.38881 to 84.70087, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.2409 - r2_score: -0.1064 - val_loss: 84.7009 - val_r2_score: -0.1020\n",
      "Epoch 16/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 89.1875 - r2_score: -0.1250\n",
      "Epoch 16: val_loss improved from 84.70087 to 83.67768, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 51.6858 - r2_score: -0.0088 - val_loss: 83.6777 - val_r2_score: -0.0887\n",
      "Epoch 17/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 13.4199 - r2_score: -1.2179\n",
      "Epoch 17: val_loss improved from 83.67768 to 83.02131, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 37.6868 - r2_score: -0.0989 - val_loss: 83.0213 - val_r2_score: -0.0802\n",
      "Epoch 18/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 47.7542 - r2_score: -0.1900\n",
      "Epoch 18: val_loss improved from 83.02131 to 82.32099, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 46.9888 - r2_score: -0.0148 - val_loss: 82.3210 - val_r2_score: -0.0711\n",
      "Epoch 19/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 79.2482 - r2_score: -0.0907\n",
      "Epoch 19: val_loss improved from 82.32099 to 81.80566, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 45.5757 - r2_score: 0.0451 - val_loss: 81.8057 - val_r2_score: -0.0644\n",
      "Epoch 20/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 41.4100 - r2_score: 0.0628\n",
      "Epoch 20: val_loss improved from 81.80566 to 81.04372, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.5201 - r2_score: 0.0486 - val_loss: 81.0437 - val_r2_score: -0.0545\n",
      "Epoch 21/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 33.0249 - r2_score: 0.0629\n",
      "Epoch 21: val_loss improved from 81.04372 to 80.40849, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 36.0216 - r2_score: 0.0738 - val_loss: 80.4085 - val_r2_score: -0.0462\n",
      "Epoch 22/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 52.4337 - r2_score: 0.0414\n",
      "Epoch 22: val_loss improved from 80.40849 to 79.69518, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41.7959 - r2_score: 0.0832 - val_loss: 79.6952 - val_r2_score: -0.0369\n",
      "Epoch 23/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.5734 - r2_score: 0.0954\n",
      "Epoch 23: val_loss improved from 79.69518 to 79.03056, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 28.8440 - r2_score: 0.1061 - val_loss: 79.0306 - val_r2_score: -0.0283\n",
      "Epoch 24/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 49.7405 - r2_score: -0.1120\n",
      "Epoch 24: val_loss improved from 79.03056 to 78.15066, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 35.3221 - r2_score: 0.0675 - val_loss: 78.1507 - val_r2_score: -0.0168\n",
      "Epoch 25/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 47.6384 - r2_score: 0.1528\n",
      "Epoch 25: val_loss improved from 78.15066 to 77.48269, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 33.7603 - r2_score: 0.1456 - val_loss: 77.4827 - val_r2_score: -0.0081\n",
      "Epoch 26/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 17.5190 - r2_score: 0.2399\n",
      "Epoch 26: val_loss improved from 77.48269 to 76.88203, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 28.0053 - r2_score: 0.1666 - val_loss: 76.8820 - val_r2_score: -3.0959e-04\n",
      "Epoch 27/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 95.3204 - r2_score: 0.0264\n",
      "Epoch 27: val_loss improved from 76.88203 to 76.06676, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 50.5828 - r2_score: 0.1153 - val_loss: 76.0668 - val_r2_score: 0.0103\n",
      "Epoch 28/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 19.2874 - r2_score: -0.0802\n",
      "Epoch 28: val_loss improved from 76.06676 to 75.44046, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 34.5178 - r2_score: 0.1192 - val_loss: 75.4405 - val_r2_score: 0.0184\n",
      "Epoch 29/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 28.1730 - r2_score: 0.1677\n",
      "Epoch 29: val_loss improved from 75.44046 to 74.85195, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 30.7632 - r2_score: 0.1880 - val_loss: 74.8520 - val_r2_score: 0.0261\n",
      "Epoch 30/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 23.6861 - r2_score: 0.2789\n",
      "Epoch 30: val_loss improved from 74.85195 to 74.06194, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32.2353 - r2_score: 0.2077 - val_loss: 74.0619 - val_r2_score: 0.0364\n",
      "Epoch 31/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 37.7815 - r2_score: 0.1553\n",
      "Epoch 31: val_loss improved from 74.06194 to 73.37892, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 35.3251 - r2_score: 0.1801 - val_loss: 73.3789 - val_r2_score: 0.0453\n",
      "Epoch 32/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 42.3712 - r2_score: 0.1369\n",
      "Epoch 32: val_loss improved from 73.37892 to 72.81156, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32.5482 - r2_score: 0.2095 - val_loss: 72.8116 - val_r2_score: 0.0527\n",
      "Epoch 33/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 29.1633 - r2_score: 0.2322\n",
      "Epoch 33: val_loss improved from 72.81156 to 72.14308, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.9100 - r2_score: 0.2340 - val_loss: 72.1431 - val_r2_score: 0.0613\n",
      "Epoch 34/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 14.8381 - r2_score: 0.3678\n",
      "Epoch 34: val_loss improved from 72.14308 to 71.45548, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 27.0581 - r2_score: 0.2704 - val_loss: 71.4555 - val_r2_score: 0.0703\n",
      "Epoch 35/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 55.4708 - r2_score: 0.1788\n",
      "Epoch 35: val_loss improved from 71.45548 to 70.84924, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 42.2223 - r2_score: 0.2011 - val_loss: 70.8492 - val_r2_score: 0.0782\n",
      "Epoch 36/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 4.8838 - r2_score: 0.5198\n",
      "Epoch 36: val_loss improved from 70.84924 to 70.39213, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23.5104 - r2_score: 0.3031 - val_loss: 70.3921 - val_r2_score: 0.0841\n",
      "Epoch 37/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 22.7473 - r2_score: 0.2964\n",
      "Epoch 37: val_loss improved from 70.39213 to 69.65070, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 31.0396 - r2_score: 0.2650 - val_loss: 69.6507 - val_r2_score: 0.0938\n",
      "Epoch 38/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.1636 - r2_score: 0.4585\n",
      "Epoch 38: val_loss improved from 69.65070 to 69.03063, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 25.2838 - r2_score: 0.3141 - val_loss: 69.0306 - val_r2_score: 0.1018\n",
      "Epoch 39/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 71.7650 - r2_score: 0.0204\n",
      "Epoch 39: val_loss improved from 69.03063 to 68.49663, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 39.7720 - r2_score: 0.2248 - val_loss: 68.4966 - val_r2_score: 0.1088\n",
      "Epoch 40/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 28.3730 - r2_score: 0.3385\n",
      "Epoch 40: val_loss improved from 68.49663 to 68.13926, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 33.1320 - r2_score: 0.2854 - val_loss: 68.1393 - val_r2_score: 0.1134\n",
      "Epoch 41/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 16.7163 - r2_score: 0.3543\n",
      "Epoch 41: val_loss improved from 68.13926 to 67.77728, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23.9603 - r2_score: 0.3376 - val_loss: 67.7773 - val_r2_score: 0.1182\n",
      "Epoch 42/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 29.5180 - r2_score: 0.2150\n",
      "Epoch 42: val_loss improved from 67.77728 to 67.26006, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.5320 - r2_score: 0.3102 - val_loss: 67.2601 - val_r2_score: 0.1249\n",
      "Epoch 43/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 20.1251 - r2_score: 0.2953\n",
      "Epoch 43: val_loss improved from 67.26006 to 66.70506, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 33.2422 - r2_score: 0.2775 - val_loss: 66.7051 - val_r2_score: 0.1321\n",
      "Epoch 44/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 50.6446 - r2_score: 0.1782\n",
      "Epoch 44: val_loss improved from 66.70506 to 66.30845, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 34.0152 - r2_score: 0.2770 - val_loss: 66.3084 - val_r2_score: 0.1373\n",
      "Epoch 45/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 31.1555 - r2_score: 0.2189\n",
      "Epoch 45: val_loss improved from 66.30845 to 65.94063, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 22.9303 - r2_score: 0.3356 - val_loss: 65.9406 - val_r2_score: 0.1420\n",
      "Epoch 46/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 38.5612 - r2_score: 0.1040\n",
      "Epoch 46: val_loss improved from 65.94063 to 65.46429, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 32.2775 - r2_score: 0.2965 - val_loss: 65.4643 - val_r2_score: 0.1482\n",
      "Epoch 47/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 30.2926 - r2_score: 0.3762\n",
      "Epoch 47: val_loss improved from 65.46429 to 65.06693, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 35.7406 - r2_score: 0.3064 - val_loss: 65.0669 - val_r2_score: 0.1534\n",
      "Epoch 48/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.2704 - r2_score: 0.5603\n",
      "Epoch 48: val_loss improved from 65.06693 to 64.75152, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 27.2466 - r2_score: 0.3620 - val_loss: 64.7515 - val_r2_score: 0.1575\n",
      "Epoch 49/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 42.4769 - r2_score: 0.3047\n",
      "Epoch 49: val_loss improved from 64.75152 to 64.49355, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.6862 - r2_score: 0.3501 - val_loss: 64.4936 - val_r2_score: 0.1609\n",
      "Epoch 50/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 50.4006 - r2_score: 0.1108\n",
      "Epoch 50: val_loss improved from 64.49355 to 64.10673, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 34.4193 - r2_score: 0.2981 - val_loss: 64.1067 - val_r2_score: 0.1659\n",
      "Epoch 51/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 10.3160 - r2_score: 0.4469\n",
      "Epoch 51: val_loss improved from 64.10673 to 63.78316, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 18.6512 - r2_score: 0.3990 - val_loss: 63.7832 - val_r2_score: 0.1701\n",
      "Epoch 52/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 66.4794 - r2_score: 0.1978\n",
      "Epoch 52: val_loss improved from 63.78316 to 63.45583, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 35.0860 - r2_score: 0.3381 - val_loss: 63.4558 - val_r2_score: 0.1744\n",
      "Epoch 53/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 14.5023 - r2_score: 0.5140\n",
      "Epoch 53: val_loss improved from 63.45583 to 63.19459, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 21.7395 - r2_score: 0.4160 - val_loss: 63.1946 - val_r2_score: 0.1778\n",
      "Epoch 54/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.4397 - r2_score: 0.6208\n",
      "Epoch 54: val_loss improved from 63.19459 to 62.84795, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 17.5661 - r2_score: 0.4427 - val_loss: 62.8479 - val_r2_score: 0.1823\n",
      "Epoch 55/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.6404 - r2_score: 0.3909\n",
      "Epoch 55: val_loss improved from 62.84795 to 62.51826, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 24.1904 - r2_score: 0.3776 - val_loss: 62.5183 - val_r2_score: 0.1866\n",
      "Epoch 56/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 12.3677 - r2_score: 0.5717\n",
      "Epoch 56: val_loss improved from 62.51826 to 62.28887, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 25.7025 - r2_score: 0.4054 - val_loss: 62.2889 - val_r2_score: 0.1896\n",
      "Epoch 57/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 20.7620 - r2_score: 0.2054\n",
      "Epoch 57: val_loss improved from 62.28887 to 62.05780, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 27.8818 - r2_score: 0.3469 - val_loss: 62.0578 - val_r2_score: 0.1926\n",
      "Epoch 58/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.2137 - r2_score: 0.2906\n",
      "Epoch 58: val_loss improved from 62.05780 to 61.79036, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 20.5236 - r2_score: 0.4038 - val_loss: 61.7904 - val_r2_score: 0.1960\n",
      "Epoch 59/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 52.5982 - r2_score: 0.2682\n",
      "Epoch 59: val_loss improved from 61.79036 to 61.51915, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 27.8679 - r2_score: 0.3855 - val_loss: 61.5191 - val_r2_score: 0.1996\n",
      "Epoch 60/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 36.0564 - r2_score: 0.3331\n",
      "Epoch 60: val_loss improved from 61.51915 to 61.31182, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 26.2955 - r2_score: 0.3881 - val_loss: 61.3118 - val_r2_score: 0.2023\n",
      "Epoch 61/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 41.7617 - r2_score: 0.3350\n",
      "Epoch 61: val_loss improved from 61.31182 to 61.10155, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.8120 - r2_score: 0.3847 - val_loss: 61.1016 - val_r2_score: 0.2050\n",
      "Epoch 62/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 47.0101 - r2_score: 0.3466\n",
      "Epoch 62: val_loss improved from 61.10155 to 60.94605, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 26.5592 - r2_score: 0.4219 - val_loss: 60.9461 - val_r2_score: 0.2070\n",
      "Epoch 63/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 41.3602 - r2_score: 0.2720\n",
      "Epoch 63: val_loss improved from 60.94605 to 60.57734, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 26.5099 - r2_score: 0.4100 - val_loss: 60.5773 - val_r2_score: 0.2118\n",
      "Epoch 64/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 22.8306 - r2_score: 0.3896\n",
      "Epoch 64: val_loss improved from 60.57734 to 60.27027, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.9930 - r2_score: 0.3791 - val_loss: 60.2703 - val_r2_score: 0.2158\n",
      "Epoch 65/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 34.2577 - r2_score: 0.3462\n",
      "Epoch 65: val_loss improved from 60.27027 to 60.05236, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25.7713 - r2_score: 0.4287 - val_loss: 60.0524 - val_r2_score: 0.2187\n",
      "Epoch 66/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 24.0477 - r2_score: 0.4183\n",
      "Epoch 66: val_loss improved from 60.05236 to 59.90776, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 21.8046 - r2_score: 0.4601 - val_loss: 59.9078 - val_r2_score: 0.2205\n",
      "Epoch 67/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 55.7196 - r2_score: 0.2194\n",
      "Epoch 67: val_loss improved from 59.90776 to 59.75373, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.7252 - r2_score: 0.3936 - val_loss: 59.7537 - val_r2_score: 0.2225\n",
      "Epoch 68/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 48.9358 - r2_score: 0.3750\n",
      "Epoch 68: val_loss improved from 59.75373 to 59.56728, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 28.8196 - r2_score: 0.4300 - val_loss: 59.5673 - val_r2_score: 0.2250\n",
      "Epoch 69/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 12.7897 - r2_score: 0.5085\n",
      "Epoch 69: val_loss improved from 59.56728 to 59.42310, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 17.4279 - r2_score: 0.4869 - val_loss: 59.4231 - val_r2_score: 0.2268\n",
      "Epoch 70/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.5548 - r2_score: -0.5727\n",
      "Epoch 70: val_loss improved from 59.42310 to 59.29701, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 15.7670 - r2_score: 0.3831 - val_loss: 59.2970 - val_r2_score: 0.2285\n",
      "Epoch 71/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 55.2213 - r2_score: 0.1840\n",
      "Epoch 71: val_loss improved from 59.29701 to 59.08824, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 27.7235 - r2_score: 0.3959 - val_loss: 59.0882 - val_r2_score: 0.2312\n",
      "Epoch 72/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9740 - r2_score: 0.6188\n",
      "Epoch 72: val_loss improved from 59.08824 to 58.90600, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 20.7229 - r2_score: 0.4561 - val_loss: 58.9060 - val_r2_score: 0.2336\n",
      "Epoch 73/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 10.6926 - r2_score: 0.5740\n",
      "Epoch 73: val_loss improved from 58.90600 to 58.67837, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 20.9957 - r2_score: 0.4686 - val_loss: 58.6784 - val_r2_score: 0.2365\n",
      "Epoch 74/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.2620 - r2_score: 0.2811\n",
      "Epoch 74: val_loss improved from 58.67837 to 58.55879, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 16.2582 - r2_score: 0.4852 - val_loss: 58.5588 - val_r2_score: 0.2381\n",
      "Epoch 75/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.1854 - r2_score: 0.6328\n",
      "Epoch 75: val_loss did not improve from 58.55879\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.9245 - r2_score: 0.5281 - val_loss: 58.6470 - val_r2_score: 0.2369\n",
      "Epoch 76/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.2759 - r2_score: 0.5734\n",
      "Epoch 76: val_loss did not improve from 58.55879\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.6449 - r2_score: 0.4871 - val_loss: 58.5770 - val_r2_score: 0.2379\n",
      "Epoch 77/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 31.0083 - r2_score: 0.4182\n",
      "Epoch 77: val_loss improved from 58.55879 to 58.33171, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 20.0142 - r2_score: 0.4812 - val_loss: 58.3317 - val_r2_score: 0.2410\n",
      "Epoch 78/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 16.0248 - r2_score: 0.3330\n",
      "Epoch 78: val_loss improved from 58.33171 to 58.14549, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 26.0588 - r2_score: 0.4241 - val_loss: 58.1455 - val_r2_score: 0.2435\n",
      "Epoch 79/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 10.9774 - r2_score: 0.5884\n",
      "Epoch 79: val_loss improved from 58.14549 to 58.04811, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.9654 - r2_score: 0.4841 - val_loss: 58.0481 - val_r2_score: 0.2447\n",
      "Epoch 80/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 35.4437 - r2_score: 0.3540\n",
      "Epoch 80: val_loss improved from 58.04811 to 57.92170, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 22.2712 - r2_score: 0.4803 - val_loss: 57.9217 - val_r2_score: 0.2464\n",
      "Epoch 81/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.6332 - r2_score: 0.7420\n",
      "Epoch 81: val_loss improved from 57.92170 to 57.69793, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 16.0713 - r2_score: 0.5571 - val_loss: 57.6979 - val_r2_score: 0.2493\n",
      "Epoch 82/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.9496 - r2_score: 0.3661\n",
      "Epoch 82: val_loss improved from 57.69793 to 57.50308, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 25.7713 - r2_score: 0.4587 - val_loss: 57.5031 - val_r2_score: 0.2518\n",
      "Epoch 83/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 18.7082 - r2_score: 0.4795\n",
      "Epoch 83: val_loss improved from 57.50308 to 57.41866, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 18.0938 - r2_score: 0.4843 - val_loss: 57.4187 - val_r2_score: 0.2529\n",
      "Epoch 84/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.1930 - r2_score: 0.8047\n",
      "Epoch 84: val_loss improved from 57.41866 to 57.35410, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 13.9482 - r2_score: 0.5465 - val_loss: 57.3541 - val_r2_score: 0.2538\n",
      "Epoch 85/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 24.7123 - r2_score: 0.4031\n",
      "Epoch 85: val_loss improved from 57.35410 to 57.20251, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23.7038 - r2_score: 0.4493 - val_loss: 57.2025 - val_r2_score: 0.2557\n",
      "Epoch 86/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 39.7938 - r2_score: 0.4375\n",
      "Epoch 86: val_loss improved from 57.20251 to 57.14352, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 24.8562 - r2_score: 0.4802 - val_loss: 57.1435 - val_r2_score: 0.2565\n",
      "Epoch 87/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 51.6810 - r2_score: -0.0631\n",
      "Epoch 87: val_loss improved from 57.14352 to 57.06833, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26.4425 - r2_score: 0.3899 - val_loss: 57.0683 - val_r2_score: 0.2575\n",
      "Epoch 88/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.3255 - r2_score: 0.6434\n",
      "Epoch 88: val_loss improved from 57.06833 to 57.02063, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 22.3270 - r2_score: 0.4729 - val_loss: 57.0206 - val_r2_score: 0.2581\n",
      "Epoch 89/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 22.0846 - r2_score: 0.4863\n",
      "Epoch 89: val_loss improved from 57.02063 to 56.97721, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23.4673 - r2_score: 0.4663 - val_loss: 56.9772 - val_r2_score: 0.2587\n",
      "Epoch 90/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 48.6032 - r2_score: 0.3561\n",
      "Epoch 90: val_loss did not improve from 56.97721\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.2742 - r2_score: 0.4612 - val_loss: 57.0041 - val_r2_score: 0.2583\n",
      "Epoch 91/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 37.5430 - r2_score: 0.4326\n",
      "Epoch 91: val_loss improved from 56.97721 to 56.87701, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 26.1453 - r2_score: 0.4932 - val_loss: 56.8770 - val_r2_score: 0.2600\n",
      "Epoch 92/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 4.0623 - r2_score: 0.7566\n",
      "Epoch 92: val_loss improved from 56.87701 to 56.74309, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13.4779 - r2_score: 0.5987 - val_loss: 56.7431 - val_r2_score: 0.2617\n",
      "Epoch 93/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 15.8486 - r2_score: 0.5450\n",
      "Epoch 93: val_loss improved from 56.74309 to 56.59891, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 22.5033 - r2_score: 0.5000 - val_loss: 56.5989 - val_r2_score: 0.2636\n",
      "Epoch 94/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 22.9154 - r2_score: 0.3063\n",
      "Epoch 94: val_loss improved from 56.59891 to 56.51543, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 20.0175 - r2_score: 0.4696 - val_loss: 56.5154 - val_r2_score: 0.2647\n",
      "Epoch 95/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 35.6907 - r2_score: 0.4030\n",
      "Epoch 95: val_loss improved from 56.51543 to 56.46540, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 24.8880 - r2_score: 0.4660 - val_loss: 56.4654 - val_r2_score: 0.2653\n",
      "Epoch 96/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.4238 - r2_score: 0.8137\n",
      "Epoch 96: val_loss improved from 56.46540 to 56.42715, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 19.5991 - r2_score: 0.5348 - val_loss: 56.4272 - val_r2_score: 0.2658\n",
      "Epoch 97/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 35.3250 - r2_score: 0.2575\n",
      "Epoch 97: val_loss improved from 56.42715 to 56.37557, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22.2248 - r2_score: 0.4673 - val_loss: 56.3756 - val_r2_score: 0.2665\n",
      "Epoch 98/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 10.6558 - r2_score: 0.4068\n",
      "Epoch 98: val_loss improved from 56.37557 to 56.31823, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.8578 - r2_score: 0.5134 - val_loss: 56.3182 - val_r2_score: 0.2672\n",
      "Epoch 99/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 12.2564 - r2_score: 0.3171\n",
      "Epoch 99: val_loss improved from 56.31823 to 56.23509, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 22.4359 - r2_score: 0.4648 - val_loss: 56.2351 - val_r2_score: 0.2683\n",
      "Epoch 100/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 61.1001 - r2_score: 0.2668\n",
      "Epoch 100: val_loss improved from 56.23509 to 56.17708, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 27.9067 - r2_score: 0.4703 - val_loss: 56.1771 - val_r2_score: 0.2691\n",
      "Epoch 101/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.1124 - r2_score: 0.7744\n",
      "Epoch 101: val_loss improved from 56.17708 to 56.16509, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.1378 - r2_score: 0.5683 - val_loss: 56.1651 - val_r2_score: 0.2692\n",
      "Epoch 102/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 58.8175 - r2_score: 0.4190\n",
      "Epoch 102: val_loss improved from 56.16509 to 55.95881, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 29.1723 - r2_score: 0.4883 - val_loss: 55.9588 - val_r2_score: 0.2719\n",
      "Epoch 103/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.8636 - r2_score: 0.5548\n",
      "Epoch 103: val_loss did not improve from 55.95881\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.4435 - r2_score: 0.5477 - val_loss: 55.9760 - val_r2_score: 0.2717\n",
      "Epoch 104/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 50.1989 - r2_score: 0.3628\n",
      "Epoch 104: val_loss did not improve from 55.95881\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.8401 - r2_score: 0.4853 - val_loss: 56.0050 - val_r2_score: 0.2713\n",
      "Epoch 105/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.0631 - r2_score: -0.3009\n",
      "Epoch 105: val_loss did not improve from 55.95881\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.5618 - r2_score: 0.4592 - val_loss: 56.0051 - val_r2_score: 0.2713\n",
      "Epoch 106/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 14.2169 - r2_score: 0.5693\n",
      "Epoch 106: val_loss improved from 55.95881 to 55.92130, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 17.2941 - r2_score: 0.5467 - val_loss: 55.9213 - val_r2_score: 0.2724\n",
      "Epoch 107/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 13.1099 - r2_score: 0.5821\n",
      "Epoch 107: val_loss improved from 55.92130 to 55.75368, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23.3139 - r2_score: 0.4971 - val_loss: 55.7537 - val_r2_score: 0.2746\n",
      "Epoch 108/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.1058 - r2_score: 0.5463\n",
      "Epoch 108: val_loss improved from 55.75368 to 55.67422, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 19.9420 - r2_score: 0.5128 - val_loss: 55.6742 - val_r2_score: 0.2756\n",
      "Epoch 109/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.5477 - r2_score: 0.8585\n",
      "Epoch 109: val_loss improved from 55.67422 to 55.63372, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15.5607 - r2_score: 0.5917 - val_loss: 55.6337 - val_r2_score: 0.2762\n",
      "Epoch 110/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.7823 - r2_score: 0.6305\n",
      "Epoch 110: val_loss improved from 55.63372 to 55.57875, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15.6330 - r2_score: 0.5733 - val_loss: 55.5788 - val_r2_score: 0.2769\n",
      "Epoch 111/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 15.5493 - r2_score: 0.6400\n",
      "Epoch 111: val_loss improved from 55.57875 to 55.53101, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 17.8430 - r2_score: 0.5479 - val_loss: 55.5310 - val_r2_score: 0.2775\n",
      "Epoch 112/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 21.1342 - r2_score: 0.5615\n",
      "Epoch 112: val_loss did not improve from 55.53101\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.4923 - r2_score: 0.5497 - val_loss: 55.5603 - val_r2_score: 0.2771\n",
      "Epoch 113/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 13.8148 - r2_score: 0.5499\n",
      "Epoch 113: val_loss did not improve from 55.53101\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.1403 - r2_score: 0.5393 - val_loss: 55.5518 - val_r2_score: 0.2772\n",
      "Epoch 114/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 13.0286 - r2_score: 0.5484\n",
      "Epoch 114: val_loss improved from 55.53101 to 55.47008, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 17.6375 - r2_score: 0.5512 - val_loss: 55.4701 - val_r2_score: 0.2783\n",
      "Epoch 115/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 17.1826 - r2_score: 0.6549\n",
      "Epoch 115: val_loss improved from 55.47008 to 55.19036, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.7536 - r2_score: 0.5505 - val_loss: 55.1904 - val_r2_score: 0.2819\n",
      "Epoch 116/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 19.9216 - r2_score: 0.6069\n",
      "Epoch 116: val_loss improved from 55.19036 to 55.13913, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.7288 - r2_score: 0.5678 - val_loss: 55.1391 - val_r2_score: 0.2826\n",
      "Epoch 117/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.7553 - r2_score: 0.6399\n",
      "Epoch 117: val_loss improved from 55.13913 to 55.08951, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.0605 - r2_score: 0.5896 - val_loss: 55.0895 - val_r2_score: 0.2832\n",
      "Epoch 118/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 10.6270 - r2_score: 0.6217\n",
      "Epoch 118: val_loss did not improve from 55.08951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.3993 - r2_score: 0.5609 - val_loss: 55.1079 - val_r2_score: 0.2830\n",
      "Epoch 119/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 31.1317 - r2_score: 0.4688\n",
      "Epoch 119: val_loss did not improve from 55.08951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.2051 - r2_score: 0.5205 - val_loss: 55.1733 - val_r2_score: 0.2821\n",
      "Epoch 120/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 31.5256 - r2_score: 0.5126\n",
      "Epoch 120: val_loss did not improve from 55.08951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.9431 - r2_score: 0.5184 - val_loss: 55.1189 - val_r2_score: 0.2828\n",
      "Epoch 121/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 24.0154 - r2_score: 0.4893\n",
      "Epoch 121: val_loss did not improve from 55.08951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.0541 - r2_score: 0.5003 - val_loss: 55.1329 - val_r2_score: 0.2827\n",
      "Epoch 122/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 17.3615 - r2_score: 0.6046\n",
      "Epoch 122: val_loss did not improve from 55.08951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.9848 - r2_score: 0.5399 - val_loss: 55.1454 - val_r2_score: 0.2825\n",
      "Epoch 123/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.0045 - r2_score: 0.5489\n",
      "Epoch 123: val_loss improved from 55.08951 to 55.08287, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.8372 - r2_score: 0.5444 - val_loss: 55.0829 - val_r2_score: 0.2833\n",
      "Epoch 124/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 36.3911 - r2_score: 0.4290\n",
      "Epoch 124: val_loss improved from 55.08287 to 54.99319, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25.1137 - r2_score: 0.5129 - val_loss: 54.9932 - val_r2_score: 0.2845\n",
      "Epoch 125/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 22.5835 - r2_score: 0.4945\n",
      "Epoch 125: val_loss improved from 54.99319 to 54.85668, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23.5588 - r2_score: 0.5232 - val_loss: 54.8567 - val_r2_score: 0.2863\n",
      "Epoch 126/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.9797 - r2_score: 0.7224\n",
      "Epoch 126: val_loss improved from 54.85668 to 54.80694, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 21.4207 - r2_score: 0.5633 - val_loss: 54.8069 - val_r2_score: 0.2869\n",
      "Epoch 127/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 10.7219 - r2_score: 0.6746\n",
      "Epoch 127: val_loss improved from 54.80694 to 54.80007, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 21.5355 - r2_score: 0.5442 - val_loss: 54.8001 - val_r2_score: 0.2870\n",
      "Epoch 128/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.3179 - r2_score: 0.7894\n",
      "Epoch 128: val_loss did not improve from 54.80007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.4444 - r2_score: 0.6392 - val_loss: 54.9454 - val_r2_score: 0.2851\n",
      "Epoch 129/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 10.7588 - r2_score: 0.5542\n",
      "Epoch 129: val_loss did not improve from 54.80007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.3590 - r2_score: 0.5623 - val_loss: 55.0631 - val_r2_score: 0.2836\n",
      "Epoch 130/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.4093 - r2_score: 0.6494\n",
      "Epoch 130: val_loss did not improve from 54.80007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.3217 - r2_score: 0.5901 - val_loss: 55.0333 - val_r2_score: 0.2840\n",
      "Epoch 131/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.3403 - r2_score: 0.2489\n",
      "Epoch 131: val_loss did not improve from 54.80007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0817 - r2_score: 0.5288 - val_loss: 54.8903 - val_r2_score: 0.2858\n",
      "Epoch 132/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 9.1415 - r2_score: 0.4088\n",
      "Epoch 132: val_loss did not improve from 54.80007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.9069 - r2_score: 0.5328 - val_loss: 54.8143 - val_r2_score: 0.2868\n",
      "Epoch 133/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 12.2482 - r2_score: 0.6008\n",
      "Epoch 133: val_loss did not improve from 54.80007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.6115 - r2_score: 0.5823 - val_loss: 54.8060 - val_r2_score: 0.2869\n",
      "Epoch 134/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 26.2065 - r2_score: 0.5518\n",
      "Epoch 134: val_loss improved from 54.80007 to 54.72547, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.9367 - r2_score: 0.5335 - val_loss: 54.7255 - val_r2_score: 0.2880\n",
      "Epoch 135/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.5900 - r2_score: 0.4939\n",
      "Epoch 135: val_loss improved from 54.72547 to 54.59245, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15.9239 - r2_score: 0.5565 - val_loss: 54.5924 - val_r2_score: 0.2897\n",
      "Epoch 136/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 12.3980 - r2_score: 0.1623\n",
      "Epoch 136: val_loss did not improve from 54.59245\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.9270 - r2_score: 0.5054 - val_loss: 54.6292 - val_r2_score: 0.2892\n",
      "Epoch 137/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.9895 - r2_score: 0.6967\n",
      "Epoch 137: val_loss did not improve from 54.59245\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.2550 - r2_score: 0.5728 - val_loss: 54.6230 - val_r2_score: 0.2893\n",
      "Epoch 138/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0839 - r2_score: 0.5020\n",
      "Epoch 138: val_loss improved from 54.59245 to 54.56287, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.3636 - r2_score: 0.5463 - val_loss: 54.5629 - val_r2_score: 0.2901\n",
      "Epoch 139/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.6502 - r2_score: 0.4949\n",
      "Epoch 139: val_loss improved from 54.56287 to 54.54907, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14.2637 - r2_score: 0.5038 - val_loss: 54.5491 - val_r2_score: 0.2903\n",
      "Epoch 140/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 23.1030 - r2_score: 0.6009\n",
      "Epoch 140: val_loss did not improve from 54.54907\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.9488 - r2_score: 0.6129 - val_loss: 54.5781 - val_r2_score: 0.2899\n",
      "Epoch 141/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 23.5209 - r2_score: 0.5652\n",
      "Epoch 141: val_loss did not improve from 54.54907\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.9076 - r2_score: 0.5796 - val_loss: 54.6914 - val_r2_score: 0.2884\n",
      "Epoch 142/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 4.7019 - r2_score: 0.1559\n",
      "Epoch 142: val_loss did not improve from 54.54907\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.7764 - r2_score: 0.4890 - val_loss: 54.6983 - val_r2_score: 0.2883\n",
      "Epoch 143/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 11.0609 - r2_score: 0.4063\n",
      "Epoch 143: val_loss did not improve from 54.54907\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.1333 - r2_score: 0.5055 - val_loss: 54.6287 - val_r2_score: 0.2892\n",
      "Epoch 144/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.3400 - r2_score: 0.4802\n",
      "Epoch 144: val_loss improved from 54.54907 to 54.53783, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13.2505 - r2_score: 0.5605 - val_loss: 54.5378 - val_r2_score: 0.2904\n",
      "Epoch 145/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.5257 - r2_score: 0.7000\n",
      "Epoch 145: val_loss did not improve from 54.53783\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.8008 - r2_score: 0.5723 - val_loss: 54.6444 - val_r2_score: 0.2890\n",
      "Epoch 146/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.3821 - r2_score: 0.3983\n",
      "Epoch 146: val_loss did not improve from 54.53783\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.2226 - r2_score: 0.5392 - val_loss: 54.7465 - val_r2_score: 0.2877\n",
      "Epoch 147/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 36.4010 - r2_score: 0.3943\n",
      "Epoch 147: val_loss did not improve from 54.53783\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.5816 - r2_score: 0.5435 - val_loss: 54.6405 - val_r2_score: 0.2891\n",
      "Epoch 148/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 10.6642 - r2_score: 0.3405\n",
      "Epoch 148: val_loss did not improve from 54.53783\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.4444 - r2_score: 0.5560 - val_loss: 54.5879 - val_r2_score: 0.2898\n",
      "Epoch 149/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 25.3036 - r2_score: 0.5884\n",
      "Epoch 149: val_loss improved from 54.53783 to 54.52933, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 20.0391 - r2_score: 0.5701 - val_loss: 54.5293 - val_r2_score: 0.2905\n",
      "Epoch 150/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.2359 - r2_score: 0.5459\n",
      "Epoch 150: val_loss did not improve from 54.52933\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.7509 - r2_score: 0.5360 - val_loss: 54.5479 - val_r2_score: 0.2903\n",
      "Epoch 151/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 26.8489 - r2_score: 0.5761\n",
      "Epoch 151: val_loss did not improve from 54.52933\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.0746 - r2_score: 0.5758 - val_loss: 54.6061 - val_r2_score: 0.2895\n",
      "Epoch 152/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.7423 - r2_score: 0.5925\n",
      "Epoch 152: val_loss did not improve from 54.52933\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.2567 - r2_score: 0.5401 - val_loss: 54.6204 - val_r2_score: 0.2893\n",
      "Epoch 153/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 11.8299 - r2_score: 0.3424\n",
      "Epoch 153: val_loss did not improve from 54.52933\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.4347 - r2_score: 0.5335 - val_loss: 54.6617 - val_r2_score: 0.2888\n",
      "Epoch 154/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 26.3775 - r2_score: 0.5614\n",
      "Epoch 154: val_loss improved from 54.52933 to 54.48042, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 19.1097 - r2_score: 0.5861 - val_loss: 54.4804 - val_r2_score: 0.2912\n",
      "Epoch 155/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 37.1955 - r2_score: 0.4914\n",
      "Epoch 155: val_loss improved from 54.48042 to 54.44542, saving model to weights.best.from_scratch.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 20.1661 - r2_score: 0.5792 - val_loss: 54.4454 - val_r2_score: 0.2916\n",
      "Epoch 156/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 17.4992 - r2_score: 0.5804\n",
      "Epoch 156: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.2948 - r2_score: 0.5681 - val_loss: 54.5260 - val_r2_score: 0.2906\n",
      "Epoch 157/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.6864 - r2_score: 0.7763\n",
      "Epoch 157: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.8658 - r2_score: 0.6150 - val_loss: 54.5711 - val_r2_score: 0.2900\n",
      "Epoch 158/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.2234 - r2_score: 0.7598\n",
      "Epoch 158: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.3126 - r2_score: 0.5918 - val_loss: 54.6729 - val_r2_score: 0.2887\n",
      "Epoch 159/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.7097 - r2_score: 0.6894\n",
      "Epoch 159: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.8346 - r2_score: 0.6056 - val_loss: 54.7519 - val_r2_score: 0.2876\n",
      "Epoch 160/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 20.1187 - r2_score: 0.5660\n",
      "Epoch 160: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.0249 - r2_score: 0.5698 - val_loss: 54.7040 - val_r2_score: 0.2882\n",
      "Epoch 161/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 12.8155 - r2_score: 0.4241\n",
      "Epoch 161: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.4150 - r2_score: 0.5476 - val_loss: 54.7449 - val_r2_score: 0.2877\n",
      "Epoch 162/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 53.8528 - r2_score: 0.4828\n",
      "Epoch 162: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.5071 - r2_score: 0.5546 - val_loss: 54.6480 - val_r2_score: 0.2890\n",
      "Epoch 163/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.7217 - r2_score: 0.7886\n",
      "Epoch 163: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.2359 - r2_score: 0.6200 - val_loss: 54.6012 - val_r2_score: 0.2896\n",
      "Epoch 164/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 4.1575 - r2_score: 0.7805\n",
      "Epoch 164: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.6436 - r2_score: 0.6256 - val_loss: 54.5897 - val_r2_score: 0.2897\n",
      "Epoch 165/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.4654 - r2_score: 0.7065\n",
      "Epoch 165: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.1116 - r2_score: 0.5815 - val_loss: 54.5777 - val_r2_score: 0.2899\n",
      "Epoch 166/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 60.5533 - r2_score: 0.4240\n",
      "Epoch 166: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26.1812 - r2_score: 0.5614 - val_loss: 54.6049 - val_r2_score: 0.2895\n",
      "Epoch 167/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 10.3002 - r2_score: 0.1380\n",
      "Epoch 167: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.7166 - r2_score: 0.4902 - val_loss: 54.5706 - val_r2_score: 0.2900\n",
      "Epoch 168/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 40.6436 - r2_score: 0.1274\n",
      "Epoch 168: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.7975 - r2_score: 0.4950 - val_loss: 54.5946 - val_r2_score: 0.2897\n",
      "Epoch 169/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.5048 - r2_score: 0.5457\n",
      "Epoch 169: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.9553 - r2_score: 0.5682 - val_loss: 54.6227 - val_r2_score: 0.2893\n",
      "Epoch 170/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.9411 - r2_score: 0.8088\n",
      "Epoch 170: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.2516 - r2_score: 0.5936 - val_loss: 54.6998 - val_r2_score: 0.2883\n",
      "Epoch 171/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.4835 - r2_score: 0.7242\n",
      "Epoch 171: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.6849 - r2_score: 0.6156 - val_loss: 54.6836 - val_r2_score: 0.2885\n",
      "Epoch 172/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.6591 - r2_score: 0.5235\n",
      "Epoch 172: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.4326 - r2_score: 0.5643 - val_loss: 54.5853 - val_r2_score: 0.2898\n",
      "Epoch 173/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 34.7834 - r2_score: 0.4943\n",
      "Epoch 173: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.9318 - r2_score: 0.5616 - val_loss: 54.5649 - val_r2_score: 0.2901\n",
      "Epoch 174/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 17.3754 - r2_score: 0.5902\n",
      "Epoch 174: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.1799 - r2_score: 0.5898 - val_loss: 54.7672 - val_r2_score: 0.2874\n",
      "Epoch 175/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.6533 - r2_score: 0.6741\n",
      "Epoch 175: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.2134 - r2_score: 0.6103 - val_loss: 54.8141 - val_r2_score: 0.2868\n",
      "Epoch 176/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 14.3748 - r2_score: 0.5545\n",
      "Epoch 176: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.1720 - r2_score: 0.5989 - val_loss: 54.7130 - val_r2_score: 0.2881\n",
      "Epoch 177/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 26.0553 - r2_score: 0.5299\n",
      "Epoch 177: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.2674 - r2_score: 0.5799 - val_loss: 54.7364 - val_r2_score: 0.2878\n",
      "Epoch 178/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 27.9612 - r2_score: 0.5818\n",
      "Epoch 178: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.5395 - r2_score: 0.5690 - val_loss: 54.7407 - val_r2_score: 0.2878\n",
      "Epoch 179/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 14.0935 - r2_score: 0.6152\n",
      "Epoch 179: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.5346 - r2_score: 0.5911 - val_loss: 54.8101 - val_r2_score: 0.2869\n",
      "Epoch 180/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.1774 - r2_score: 0.7679\n",
      "Epoch 180: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.6347 - r2_score: 0.6645 - val_loss: 55.0045 - val_r2_score: 0.2843\n",
      "Epoch 181/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 4.9503 - r2_score: 0.1446\n",
      "Epoch 181: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.1949 - r2_score: 0.4742 - val_loss: 54.8896 - val_r2_score: 0.2858\n",
      "Epoch 182/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 30.5649 - r2_score: 0.4287\n",
      "Epoch 182: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.3573 - r2_score: 0.5426 - val_loss: 55.1560 - val_r2_score: 0.2824\n",
      "Epoch 183/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.3485 - r2_score: 0.6044\n",
      "Epoch 183: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.9820 - r2_score: 0.5621 - val_loss: 55.1264 - val_r2_score: 0.2828\n",
      "Epoch 184/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 17.3895 - r2_score: 0.6583\n",
      "Epoch 184: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.4811 - r2_score: 0.5912 - val_loss: 55.1583 - val_r2_score: 0.2823\n",
      "Epoch 185/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.0625 - r2_score: 0.7799\n",
      "Epoch 185: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0708 - r2_score: 0.6123 - val_loss: 55.2168 - val_r2_score: 0.2816\n",
      "Epoch 186/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.0176 - r2_score: 0.6502\n",
      "Epoch 186: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0321 - r2_score: 0.5936 - val_loss: 55.1680 - val_r2_score: 0.2822\n",
      "Epoch 187/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 16.9719 - r2_score: 0.5792\n",
      "Epoch 187: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.9662 - r2_score: 0.5774 - val_loss: 55.2217 - val_r2_score: 0.2815\n",
      "Epoch 188/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.9037 - r2_score: 0.5734\n",
      "Epoch 188: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.3697 - r2_score: 0.6133 - val_loss: 55.3873 - val_r2_score: 0.2794\n",
      "Epoch 189/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.8141 - r2_score: 0.5119\n",
      "Epoch 189: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.0926 - r2_score: 0.5881 - val_loss: 55.3250 - val_r2_score: 0.2802\n",
      "Epoch 190/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 11.5963 - r2_score: 0.5396\n",
      "Epoch 190: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.2097 - r2_score: 0.6091 - val_loss: 55.5611 - val_r2_score: 0.2771\n",
      "Epoch 191/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.6149 - r2_score: 0.3447\n",
      "Epoch 191: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.0012 - r2_score: 0.5528 - val_loss: 55.4992 - val_r2_score: 0.2779\n",
      "Epoch 192/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 26.4709 - r2_score: 0.5365\n",
      "Epoch 192: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.8207 - r2_score: 0.6179 - val_loss: 55.4228 - val_r2_score: 0.2789\n",
      "Epoch 193/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.2396 - r2_score: 0.5134\n",
      "Epoch 193: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0353 - r2_score: 0.5682 - val_loss: 55.4172 - val_r2_score: 0.2790\n",
      "Epoch 194/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 19.0454 - r2_score: 0.5861\n",
      "Epoch 194: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.2739 - r2_score: 0.6008 - val_loss: 55.3924 - val_r2_score: 0.2793\n",
      "Epoch 195/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.1914 - r2_score: 0.1640\n",
      "Epoch 195: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0297 - r2_score: 0.5385 - val_loss: 55.4746 - val_r2_score: 0.2782\n",
      "Epoch 196/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 6.7650 - r2_score: 0.7695\n",
      "Epoch 196: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.0530 - r2_score: 0.6286 - val_loss: 55.6023 - val_r2_score: 0.2766\n",
      "Epoch 197/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 54.8688 - r2_score: 0.4588\n",
      "Epoch 197: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.1896 - r2_score: 0.5758 - val_loss: 55.7449 - val_r2_score: 0.2747\n",
      "Epoch 198/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 16.4591 - r2_score: 0.5885\n",
      "Epoch 198: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.8712 - r2_score: 0.6289 - val_loss: 55.9144 - val_r2_score: 0.2725\n",
      "Epoch 199/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.3824 - r2_score: 0.6355\n",
      "Epoch 199: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.7904 - r2_score: 0.6330 - val_loss: 55.9171 - val_r2_score: 0.2725\n",
      "Epoch 200/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.2567 - r2_score: 0.7383\n",
      "Epoch 200: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.8998 - r2_score: 0.6387 - val_loss: 56.2115 - val_r2_score: 0.2686\n",
      "Epoch 201/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.4862 - r2_score: 0.6137\n",
      "Epoch 201: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.8074 - r2_score: 0.6270 - val_loss: 56.2492 - val_r2_score: 0.2681\n",
      "Epoch 202/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 30.7481 - r2_score: 0.5075\n",
      "Epoch 202: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.5562 - r2_score: 0.5929 - val_loss: 56.4355 - val_r2_score: 0.2657\n",
      "Epoch 203/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 43.8802 - r2_score: 0.4266\n",
      "Epoch 203: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.8505 - r2_score: 0.5847 - val_loss: 56.6583 - val_r2_score: 0.2628\n",
      "Epoch 204/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 27.5942 - r2_score: 0.5396\n",
      "Epoch 204: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.1024 - r2_score: 0.5757 - val_loss: 56.6097 - val_r2_score: 0.2635\n",
      "Epoch 205/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 23.4831 - r2_score: 0.5712\n",
      "Epoch 205: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.0883 - r2_score: 0.6142 - val_loss: 56.7229 - val_r2_score: 0.2620\n",
      "Epoch 206/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 15.1543 - r2_score: 0.2910\n",
      "Epoch 206: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.4489 - r2_score: 0.5695 - val_loss: 56.7158 - val_r2_score: 0.2621\n",
      "Epoch 207/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 17.1029 - r2_score: 0.6648\n",
      "Epoch 207: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.7256 - r2_score: 0.5844 - val_loss: 56.9787 - val_r2_score: 0.2587\n",
      "Epoch 208/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.0658 - r2_score: 0.6677\n",
      "Epoch 208: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.4336 - r2_score: 0.6252 - val_loss: 57.1110 - val_r2_score: 0.2569\n",
      "Epoch 209/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8747 - r2_score: 0.0594\n",
      "Epoch 209: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.4430 - r2_score: 0.5088 - val_loss: 57.0088 - val_r2_score: 0.2583\n",
      "Epoch 210/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.0669 - r2_score: 0.6683\n",
      "Epoch 210: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.9336 - r2_score: 0.6239 - val_loss: 57.2238 - val_r2_score: 0.2555\n",
      "Epoch 211/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 25.7286 - r2_score: 0.5991\n",
      "Epoch 211: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.4312 - r2_score: 0.6123 - val_loss: 57.2246 - val_r2_score: 0.2555\n",
      "Epoch 212/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 20.0444 - r2_score: 0.4587\n",
      "Epoch 212: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.8360 - r2_score: 0.5708 - val_loss: 57.3224 - val_r2_score: 0.2542\n",
      "Epoch 213/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.8597 - r2_score: 0.5266\n",
      "Epoch 213: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0214 - r2_score: 0.6275 - val_loss: 57.3022 - val_r2_score: 0.2544\n",
      "Epoch 214/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.4980 - r2_score: 0.5892\n",
      "Epoch 214: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.4450 - r2_score: 0.6383 - val_loss: 57.4924 - val_r2_score: 0.2520\n",
      "Epoch 215/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.0729 - r2_score: 0.6283\n",
      "Epoch 215: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.4396 - r2_score: 0.5991 - val_loss: 57.8116 - val_r2_score: 0.2478\n",
      "Epoch 216/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.2461 - r2_score: 0.6150\n",
      "Epoch 216: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.6070 - r2_score: 0.6012 - val_loss: 57.8032 - val_r2_score: 0.2479\n",
      "Epoch 217/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 32.6470 - r2_score: 0.6313\n",
      "Epoch 217: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.3208 - r2_score: 0.6324 - val_loss: 57.8346 - val_r2_score: 0.2475\n",
      "Epoch 218/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 33.1211 - r2_score: 0.5134\n",
      "Epoch 218: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.3052 - r2_score: 0.5970 - val_loss: 58.1030 - val_r2_score: 0.2440\n",
      "Epoch 219/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.8403 - r2_score: 0.6815\n",
      "Epoch 219: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.0995 - r2_score: 0.6543 - val_loss: 58.3782 - val_r2_score: 0.2404\n",
      "Epoch 220/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 18.3692 - r2_score: 0.5778\n",
      "Epoch 220: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.9426 - r2_score: 0.6365 - val_loss: 58.1751 - val_r2_score: 0.2431\n",
      "Epoch 221/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 18.3067 - r2_score: 0.5892\n",
      "Epoch 221: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.7367 - r2_score: 0.5970 - val_loss: 58.1078 - val_r2_score: 0.2440\n",
      "Epoch 222/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.1111 - r2_score: 0.7326\n",
      "Epoch 222: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.9933 - r2_score: 0.6426 - val_loss: 58.1972 - val_r2_score: 0.2428\n",
      "Epoch 223/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.6118 - r2_score: 0.7247\n",
      "Epoch 223: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.8209 - r2_score: 0.6664 - val_loss: 58.2296 - val_r2_score: 0.2424\n",
      "Epoch 224/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 13.0214 - r2_score: 0.6984\n",
      "Epoch 224: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.5711 - r2_score: 0.6713 - val_loss: 58.7327 - val_r2_score: 0.2358\n",
      "Epoch 225/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.9123 - r2_score: 0.7453\n",
      "Epoch 225: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.9142 - r2_score: 0.6513 - val_loss: 58.8428 - val_r2_score: 0.2344\n",
      "Epoch 226/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.3093 - r2_score: 0.6307\n",
      "Epoch 226: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.3355 - r2_score: 0.6260 - val_loss: 58.8978 - val_r2_score: 0.2337\n",
      "Epoch 227/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.5108 - r2_score: 0.8334\n",
      "Epoch 227: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.1367 - r2_score: 0.6595 - val_loss: 58.8551 - val_r2_score: 0.2342\n",
      "Epoch 228/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 27.8249 - r2_score: 0.6591\n",
      "Epoch 228: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.5617 - r2_score: 0.6338 - val_loss: 58.7810 - val_r2_score: 0.2352\n",
      "Epoch 229/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 10.1308 - r2_score: -1.0419\n",
      "Epoch 229: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.3048 - r2_score: 0.4133 - val_loss: 58.8694 - val_r2_score: 0.2341\n",
      "Epoch 230/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.2428 - r2_score: 0.6192\n",
      "Epoch 230: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.4713 - r2_score: 0.6455 - val_loss: 59.0242 - val_r2_score: 0.2320\n",
      "Epoch 231/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 36.7301 - r2_score: 0.5086\n",
      "Epoch 231: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.8758 - r2_score: 0.6171 - val_loss: 58.9981 - val_r2_score: 0.2324\n",
      "Epoch 232/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.3810 - r2_score: 0.8002\n",
      "Epoch 232: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.4954 - r2_score: 0.6928 - val_loss: 58.9358 - val_r2_score: 0.2332\n",
      "Epoch 233/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 11.6976 - r2_score: 0.6450\n",
      "Epoch 233: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.8345 - r2_score: 0.6590 - val_loss: 58.5789 - val_r2_score: 0.2378\n",
      "Epoch 234/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 12.5750 - r2_score: 0.6939\n",
      "Epoch 234: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.6267 - r2_score: 0.6653 - val_loss: 58.4834 - val_r2_score: 0.2391\n",
      "Epoch 235/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 41.6792 - r2_score: 0.4508\n",
      "Epoch 235: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.8655 - r2_score: 0.5922 - val_loss: 59.0369 - val_r2_score: 0.2319\n",
      "Epoch 236/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 33.1613 - r2_score: 0.5182\n",
      "Epoch 236: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.7471 - r2_score: 0.6118 - val_loss: 59.0008 - val_r2_score: 0.2323\n",
      "Epoch 237/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.5499 - r2_score: 0.5911\n",
      "Epoch 237: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.3545 - r2_score: 0.6259 - val_loss: 59.5102 - val_r2_score: 0.2257\n",
      "Epoch 238/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.6437 - r2_score: 0.5382\n",
      "Epoch 238: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.2872 - r2_score: 0.6186 - val_loss: 59.8843 - val_r2_score: 0.2208\n",
      "Epoch 239/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 4.8425 - r2_score: 0.7064\n",
      "Epoch 239: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.9328 - r2_score: 0.6644 - val_loss: 59.6628 - val_r2_score: 0.2237\n",
      "Epoch 240/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 14.1387 - r2_score: 0.6826\n",
      "Epoch 240: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.7818 - r2_score: 0.6467 - val_loss: 59.4139 - val_r2_score: 0.2270\n",
      "Epoch 241/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.4207 - r2_score: 0.2175\n",
      "Epoch 241: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.0128 - r2_score: 0.5927 - val_loss: 59.5283 - val_r2_score: 0.2255\n",
      "Epoch 242/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.7377 - r2_score: 0.6911\n",
      "Epoch 242: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.4680 - r2_score: 0.6687 - val_loss: 59.9400 - val_r2_score: 0.2201\n",
      "Epoch 243/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 22.4798 - r2_score: 0.6266\n",
      "Epoch 243: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.0493 - r2_score: 0.6645 - val_loss: 59.9834 - val_r2_score: 0.2196\n",
      "Epoch 244/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 25.9865 - r2_score: 0.5341\n",
      "Epoch 244: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.6406 - r2_score: 0.6514 - val_loss: 59.3975 - val_r2_score: 0.2272\n",
      "Epoch 245/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.3777 - r2_score: 0.7775\n",
      "Epoch 245: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.0212 - r2_score: 0.6922 - val_loss: 59.6413 - val_r2_score: 0.2240\n",
      "Epoch 246/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.6957 - r2_score: 0.7901\n",
      "Epoch 246: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.6523 - r2_score: 0.6884 - val_loss: 59.7982 - val_r2_score: 0.2220\n",
      "Epoch 247/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 18.9471 - r2_score: 0.6037\n",
      "Epoch 247: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.2123 - r2_score: 0.6185 - val_loss: 59.6421 - val_r2_score: 0.2240\n",
      "Epoch 248/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 17.9425 - r2_score: 0.6160\n",
      "Epoch 248: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.9487 - r2_score: 0.6421 - val_loss: 59.3934 - val_r2_score: 0.2272\n",
      "Epoch 249/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 4.5951 - r2_score: 0.8104\n",
      "Epoch 249: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5451 - r2_score: 0.7130 - val_loss: 59.3407 - val_r2_score: 0.2279\n",
      "Epoch 250/250\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.9635 - r2_score: 0.4822\n",
      "Epoch 250: val_loss did not improve from 54.44542\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3887 - r2_score: 0.6366 - val_loss: 59.3198 - val_r2_score: 0.2282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14a7b2d50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilando el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['r2_score'])\n",
    "\n",
    "\n",
    "model_epochs = 250\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='weights.best.from_scratch.keras', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(t_features_train, t_target_train, validation_data=(t_features_val, t_target_val), epochs=model_epochs, batch_size=10, callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e09ed9-1f5d-4d0e-b655-c35bd1622ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.12195122  0.39797395  0.51170312 -1.0272263  -0.01766304  6.72727273\n",
      "   8.625       0.          1.          0.          0.          1.\n",
      "   0.          1.          0.          0.          0.        ]], shape=(1, 17), dtype=float64)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "[[-9.372835]]\n"
     ]
    }
   ],
   "source": [
    "tensor_preprocessed_datapoint = tf.convert_to_tensor(sample_pre_processed_datapoint)\n",
    "print(tensor_preprocessed_datapoint)\n",
    "prediction = model.predict(tensor_preprocessed_datapoint)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3901bdc-18b6-42fc-9e6d-2c5af8ebca97",
   "metadata": {},
   "source": [
    "## Exportando modelo de Tensorflow para servirlo en produccion.\n",
    "\n",
    "Aqui exportamos el modelo ya entrenado para poderlo servir en produccion usando TensorFlow Serving. Usaremos `model.export` para que se pueda salvar en el formato SavedModel de Tensorflow el cual es el formato soportado por TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cce0c5f-95d5-4032-b5b7-327dfee7c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: barcamp2024-model-tensorflow/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: barcamp2024-model-tensorflow/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'barcamp2024-model-tensorflow/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 17), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5543462672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5543461520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5543461904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5543464016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5543461712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5543462864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "model.output_names = ['output']\n",
    "model.export('barcamp2024-model-tensorflow/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cec5d-444c-4bc9-b520-cc959496b629",
   "metadata": {},
   "source": [
    "## Entrenando modelo de deep learning con PyTorch\n",
    "\n",
    "Ahora entrenaremos la misma red neuronal con los datos ya pre-procesados pero con la libreria PyTorch de Meta.\n",
    "\n",
    "Primero procederemos a crear la clase que albergara la red neuronal asi como la clase que formateara el dataset al esquema que utiliza PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2d370c-17e9-458e-9e96-21f40bde705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(17, 15),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(15,7),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(7,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class WeightVariationDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            self.X = torch.tensor(X.values)\n",
    "            self.y = torch.tensor(y.values)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd174a2f-0c3a-4ed8-ae50-75eb28465112",
   "metadata": {},
   "source": [
    "## Preparacion de dataset de PyTorch\n",
    "\n",
    "Preparando el dataset con la clase WeightVariationDataset. Utilizaremos los datos previamente pre-procesados que utilizamos en la etapa con TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f41cf1-5231-4e3a-8470-de5d0063f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando el dataset previamente pre-procesado (no haremos nuevos preprocesamiento)\n",
    "weight_change_dataset = WeightVariationDataset(features_train, target_train)\n",
    "trainloader = torch.utils.data.DataLoader(weight_change_dataset, batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf436e44-688b-4e21-a20b-7aea7719e480",
   "metadata": {},
   "source": [
    "## Inicializando y entrenando modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8668b330-62b7-47f0-8dce-ce7520eebfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.076\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.070\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.022\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.172\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.049\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.150\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 0.114\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.196\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.072\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.175\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 0.135\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 0.114\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.028\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 0.020\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.174\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 0.149\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 0.187\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 0.167\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.053\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.145\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 0.238\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.040\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 0.166\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 0.090\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 0.106\n",
      "Starting epoch 29\n",
      "Loss after mini-batch     1: 0.015\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 0.216\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 0.146\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 0.099\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 0.034\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 0.137\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 0.112\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 0.050\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 0.180\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 0.013\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 0.131\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 0.032\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 0.105\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 0.067\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 0.046\n",
      "Starting epoch 46\n",
      "Loss after mini-batch     1: 0.165\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 0.019\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.118\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 0.070\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 0.046\n",
      "Starting epoch 51\n",
      "Loss after mini-batch     1: 0.040\n",
      "Starting epoch 52\n",
      "Loss after mini-batch     1: 0.074\n",
      "Starting epoch 53\n",
      "Loss after mini-batch     1: 0.055\n",
      "Starting epoch 54\n",
      "Loss after mini-batch     1: 0.119\n",
      "Starting epoch 55\n",
      "Loss after mini-batch     1: 0.171\n",
      "Starting epoch 56\n",
      "Loss after mini-batch     1: 0.055\n",
      "Starting epoch 57\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 58\n",
      "Loss after mini-batch     1: 0.090\n",
      "Starting epoch 59\n",
      "Loss after mini-batch     1: 0.051\n",
      "Starting epoch 60\n",
      "Loss after mini-batch     1: 0.083\n",
      "Starting epoch 61\n",
      "Loss after mini-batch     1: 0.093\n",
      "Starting epoch 62\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 63\n",
      "Loss after mini-batch     1: 0.037\n",
      "Starting epoch 64\n",
      "Loss after mini-batch     1: 0.023\n",
      "Starting epoch 65\n",
      "Loss after mini-batch     1: 0.166\n",
      "Starting epoch 66\n",
      "Loss after mini-batch     1: 0.087\n",
      "Starting epoch 67\n",
      "Loss after mini-batch     1: 0.086\n",
      "Starting epoch 68\n",
      "Loss after mini-batch     1: 0.045\n",
      "Starting epoch 69\n",
      "Loss after mini-batch     1: 0.045\n",
      "Starting epoch 70\n",
      "Loss after mini-batch     1: 0.092\n",
      "Starting epoch 71\n",
      "Loss after mini-batch     1: 0.053\n",
      "Starting epoch 72\n",
      "Loss after mini-batch     1: 0.062\n",
      "Starting epoch 73\n",
      "Loss after mini-batch     1: 0.014\n",
      "Starting epoch 74\n",
      "Loss after mini-batch     1: 0.172\n",
      "Starting epoch 75\n",
      "Loss after mini-batch     1: 0.040\n",
      "Starting epoch 76\n",
      "Loss after mini-batch     1: 0.169\n",
      "Starting epoch 77\n",
      "Loss after mini-batch     1: 0.047\n",
      "Starting epoch 78\n",
      "Loss after mini-batch     1: 0.154\n",
      "Starting epoch 79\n",
      "Loss after mini-batch     1: 0.099\n",
      "Starting epoch 80\n",
      "Loss after mini-batch     1: 0.015\n",
      "Starting epoch 81\n",
      "Loss after mini-batch     1: 0.087\n",
      "Starting epoch 82\n",
      "Loss after mini-batch     1: 0.051\n",
      "Starting epoch 83\n",
      "Loss after mini-batch     1: 0.145\n",
      "Starting epoch 84\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 85\n",
      "Loss after mini-batch     1: 0.058\n",
      "Starting epoch 86\n",
      "Loss after mini-batch     1: 0.090\n",
      "Starting epoch 87\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 88\n",
      "Loss after mini-batch     1: 0.083\n",
      "Starting epoch 89\n",
      "Loss after mini-batch     1: 0.072\n",
      "Starting epoch 90\n",
      "Loss after mini-batch     1: 0.075\n",
      "Starting epoch 91\n",
      "Loss after mini-batch     1: 0.089\n",
      "Starting epoch 92\n",
      "Loss after mini-batch     1: 0.045\n",
      "Starting epoch 93\n",
      "Loss after mini-batch     1: 0.132\n",
      "Starting epoch 94\n",
      "Loss after mini-batch     1: 0.019\n",
      "Starting epoch 95\n",
      "Loss after mini-batch     1: 0.266\n",
      "Starting epoch 96\n",
      "Loss after mini-batch     1: 0.020\n",
      "Starting epoch 97\n",
      "Loss after mini-batch     1: 0.093\n",
      "Starting epoch 98\n",
      "Loss after mini-batch     1: 0.025\n",
      "Starting epoch 99\n",
      "Loss after mini-batch     1: 0.098\n",
      "Starting epoch 100\n",
      "Loss after mini-batch     1: 0.136\n",
      "Starting epoch 101\n",
      "Loss after mini-batch     1: 0.144\n",
      "Starting epoch 102\n",
      "Loss after mini-batch     1: 0.033\n",
      "Starting epoch 103\n",
      "Loss after mini-batch     1: 0.131\n",
      "Starting epoch 104\n",
      "Loss after mini-batch     1: 0.056\n",
      "Starting epoch 105\n",
      "Loss after mini-batch     1: 0.117\n",
      "Starting epoch 106\n",
      "Loss after mini-batch     1: 0.109\n",
      "Starting epoch 107\n",
      "Loss after mini-batch     1: 0.064\n",
      "Starting epoch 108\n",
      "Loss after mini-batch     1: 0.113\n",
      "Starting epoch 109\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 110\n",
      "Loss after mini-batch     1: 0.117\n",
      "Starting epoch 111\n",
      "Loss after mini-batch     1: 0.072\n",
      "Starting epoch 112\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 113\n",
      "Loss after mini-batch     1: 0.069\n",
      "Starting epoch 114\n",
      "Loss after mini-batch     1: 0.166\n",
      "Starting epoch 115\n",
      "Loss after mini-batch     1: 0.019\n",
      "Starting epoch 116\n",
      "Loss after mini-batch     1: 0.059\n",
      "Starting epoch 117\n",
      "Loss after mini-batch     1: 0.116\n",
      "Starting epoch 118\n",
      "Loss after mini-batch     1: 0.130\n",
      "Starting epoch 119\n",
      "Loss after mini-batch     1: 0.197\n",
      "Starting epoch 120\n",
      "Loss after mini-batch     1: 0.109\n",
      "Starting epoch 121\n",
      "Loss after mini-batch     1: 0.020\n",
      "Starting epoch 122\n",
      "Loss after mini-batch     1: 0.018\n",
      "Starting epoch 123\n",
      "Loss after mini-batch     1: 0.068\n",
      "Starting epoch 124\n",
      "Loss after mini-batch     1: 0.074\n",
      "Starting epoch 125\n",
      "Loss after mini-batch     1: 0.117\n",
      "Starting epoch 126\n",
      "Loss after mini-batch     1: 0.046\n",
      "Starting epoch 127\n",
      "Loss after mini-batch     1: 0.071\n",
      "Starting epoch 128\n",
      "Loss after mini-batch     1: 0.132\n",
      "Starting epoch 129\n",
      "Loss after mini-batch     1: 0.041\n",
      "Starting epoch 130\n",
      "Loss after mini-batch     1: 0.059\n",
      "Starting epoch 131\n",
      "Loss after mini-batch     1: 0.111\n",
      "Starting epoch 132\n",
      "Loss after mini-batch     1: 0.242\n",
      "Starting epoch 133\n",
      "Loss after mini-batch     1: 0.105\n",
      "Starting epoch 134\n",
      "Loss after mini-batch     1: 0.055\n",
      "Starting epoch 135\n",
      "Loss after mini-batch     1: 0.075\n",
      "Starting epoch 136\n",
      "Loss after mini-batch     1: 0.050\n",
      "Starting epoch 137\n",
      "Loss after mini-batch     1: 0.126\n",
      "Starting epoch 138\n",
      "Loss after mini-batch     1: 0.084\n",
      "Starting epoch 139\n",
      "Loss after mini-batch     1: 0.052\n",
      "Starting epoch 140\n",
      "Loss after mini-batch     1: 0.025\n",
      "Starting epoch 141\n",
      "Loss after mini-batch     1: 0.137\n",
      "Starting epoch 142\n",
      "Loss after mini-batch     1: 0.096\n",
      "Starting epoch 143\n",
      "Loss after mini-batch     1: 0.033\n",
      "Starting epoch 144\n",
      "Loss after mini-batch     1: 0.093\n",
      "Starting epoch 145\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 146\n",
      "Loss after mini-batch     1: 0.114\n",
      "Starting epoch 147\n",
      "Loss after mini-batch     1: 0.159\n",
      "Starting epoch 148\n",
      "Loss after mini-batch     1: 0.103\n",
      "Starting epoch 149\n",
      "Loss after mini-batch     1: 0.057\n",
      "Starting epoch 150\n",
      "Loss after mini-batch     1: 0.075\n",
      "Starting epoch 151\n",
      "Loss after mini-batch     1: 0.176\n",
      "Starting epoch 152\n",
      "Loss after mini-batch     1: 0.108\n",
      "Starting epoch 153\n",
      "Loss after mini-batch     1: 0.090\n",
      "Starting epoch 154\n",
      "Loss after mini-batch     1: 0.028\n",
      "Starting epoch 155\n",
      "Loss after mini-batch     1: 0.097\n",
      "Starting epoch 156\n",
      "Loss after mini-batch     1: 0.069\n",
      "Starting epoch 157\n",
      "Loss after mini-batch     1: 0.038\n",
      "Starting epoch 158\n",
      "Loss after mini-batch     1: 0.058\n",
      "Starting epoch 159\n",
      "Loss after mini-batch     1: 0.111\n",
      "Starting epoch 160\n",
      "Loss after mini-batch     1: 0.102\n",
      "Starting epoch 161\n",
      "Loss after mini-batch     1: 0.045\n",
      "Starting epoch 162\n",
      "Loss after mini-batch     1: 0.051\n",
      "Starting epoch 163\n",
      "Loss after mini-batch     1: 0.137\n",
      "Starting epoch 164\n",
      "Loss after mini-batch     1: 0.083\n",
      "Starting epoch 165\n",
      "Loss after mini-batch     1: 0.172\n",
      "Starting epoch 166\n",
      "Loss after mini-batch     1: 0.063\n",
      "Starting epoch 167\n",
      "Loss after mini-batch     1: 0.132\n",
      "Starting epoch 168\n",
      "Loss after mini-batch     1: 0.109\n",
      "Starting epoch 169\n",
      "Loss after mini-batch     1: 0.085\n",
      "Starting epoch 170\n",
      "Loss after mini-batch     1: 0.045\n",
      "Starting epoch 171\n",
      "Loss after mini-batch     1: 0.100\n",
      "Starting epoch 172\n",
      "Loss after mini-batch     1: 0.040\n",
      "Starting epoch 173\n",
      "Loss after mini-batch     1: 0.036\n",
      "Starting epoch 174\n",
      "Loss after mini-batch     1: 0.107\n",
      "Starting epoch 175\n",
      "Loss after mini-batch     1: 0.045\n",
      "Starting epoch 176\n",
      "Loss after mini-batch     1: 0.059\n",
      "Starting epoch 177\n",
      "Loss after mini-batch     1: 0.170\n",
      "Starting epoch 178\n",
      "Loss after mini-batch     1: 0.180\n",
      "Starting epoch 179\n",
      "Loss after mini-batch     1: 0.325\n",
      "Starting epoch 180\n",
      "Loss after mini-batch     1: 0.203\n",
      "Starting epoch 181\n",
      "Loss after mini-batch     1: 0.030\n",
      "Starting epoch 182\n",
      "Loss after mini-batch     1: 0.138\n",
      "Starting epoch 183\n",
      "Loss after mini-batch     1: 0.123\n",
      "Starting epoch 184\n",
      "Loss after mini-batch     1: 0.056\n",
      "Starting epoch 185\n",
      "Loss after mini-batch     1: 0.023\n",
      "Starting epoch 186\n",
      "Loss after mini-batch     1: 0.040\n",
      "Starting epoch 187\n",
      "Loss after mini-batch     1: 0.113\n",
      "Starting epoch 188\n",
      "Loss after mini-batch     1: 0.054\n",
      "Starting epoch 189\n",
      "Loss after mini-batch     1: 0.090\n",
      "Starting epoch 190\n",
      "Loss after mini-batch     1: 0.094\n",
      "Starting epoch 191\n",
      "Loss after mini-batch     1: 0.066\n",
      "Starting epoch 192\n",
      "Loss after mini-batch     1: 0.123\n",
      "Starting epoch 193\n",
      "Loss after mini-batch     1: 0.175\n",
      "Starting epoch 194\n",
      "Loss after mini-batch     1: 0.168\n",
      "Starting epoch 195\n",
      "Loss after mini-batch     1: 0.175\n",
      "Starting epoch 196\n",
      "Loss after mini-batch     1: 0.131\n",
      "Starting epoch 197\n",
      "Loss after mini-batch     1: 0.111\n",
      "Starting epoch 198\n",
      "Loss after mini-batch     1: 0.075\n",
      "Starting epoch 199\n",
      "Loss after mini-batch     1: 0.092\n",
      "Starting epoch 200\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 201\n",
      "Loss after mini-batch     1: 0.099\n",
      "Starting epoch 202\n",
      "Loss after mini-batch     1: 0.068\n",
      "Starting epoch 203\n",
      "Loss after mini-batch     1: 0.057\n",
      "Starting epoch 204\n",
      "Loss after mini-batch     1: 0.035\n",
      "Starting epoch 205\n",
      "Loss after mini-batch     1: 0.072\n",
      "Starting epoch 206\n",
      "Loss after mini-batch     1: 0.129\n",
      "Starting epoch 207\n",
      "Loss after mini-batch     1: 0.033\n",
      "Starting epoch 208\n",
      "Loss after mini-batch     1: 0.035\n",
      "Starting epoch 209\n",
      "Loss after mini-batch     1: 0.127\n",
      "Starting epoch 210\n",
      "Loss after mini-batch     1: 0.055\n",
      "Starting epoch 211\n",
      "Loss after mini-batch     1: 0.149\n",
      "Starting epoch 212\n",
      "Loss after mini-batch     1: 0.075\n",
      "Starting epoch 213\n",
      "Loss after mini-batch     1: 0.050\n",
      "Starting epoch 214\n",
      "Loss after mini-batch     1: 0.028\n",
      "Starting epoch 215\n",
      "Loss after mini-batch     1: 0.029\n",
      "Starting epoch 216\n",
      "Loss after mini-batch     1: 0.059\n",
      "Starting epoch 217\n",
      "Loss after mini-batch     1: 0.055\n",
      "Starting epoch 218\n",
      "Loss after mini-batch     1: 0.036\n",
      "Starting epoch 219\n",
      "Loss after mini-batch     1: 0.063\n",
      "Starting epoch 220\n",
      "Loss after mini-batch     1: 0.026\n",
      "Starting epoch 221\n",
      "Loss after mini-batch     1: 0.116\n",
      "Starting epoch 222\n",
      "Loss after mini-batch     1: 0.028\n",
      "Starting epoch 223\n",
      "Loss after mini-batch     1: 0.132\n",
      "Starting epoch 224\n",
      "Loss after mini-batch     1: 0.033\n",
      "Starting epoch 225\n",
      "Loss after mini-batch     1: 0.158\n",
      "Starting epoch 226\n",
      "Loss after mini-batch     1: 0.029\n",
      "Starting epoch 227\n",
      "Loss after mini-batch     1: 0.064\n",
      "Starting epoch 228\n",
      "Loss after mini-batch     1: 0.047\n",
      "Starting epoch 229\n",
      "Loss after mini-batch     1: 0.049\n",
      "Starting epoch 230\n",
      "Loss after mini-batch     1: 0.104\n",
      "Starting epoch 231\n",
      "Loss after mini-batch     1: 0.070\n",
      "Starting epoch 232\n",
      "Loss after mini-batch     1: 0.129\n",
      "Starting epoch 233\n",
      "Loss after mini-batch     1: 0.058\n",
      "Starting epoch 234\n",
      "Loss after mini-batch     1: 0.093\n",
      "Starting epoch 235\n",
      "Loss after mini-batch     1: 0.033\n",
      "Starting epoch 236\n",
      "Loss after mini-batch     1: 0.105\n",
      "Starting epoch 237\n",
      "Loss after mini-batch     1: 0.041\n",
      "Starting epoch 238\n",
      "Loss after mini-batch     1: 0.070\n",
      "Starting epoch 239\n",
      "Loss after mini-batch     1: 0.145\n",
      "Starting epoch 240\n",
      "Loss after mini-batch     1: 0.112\n",
      "Starting epoch 241\n",
      "Loss after mini-batch     1: 0.059\n",
      "Starting epoch 242\n",
      "Loss after mini-batch     1: 0.041\n",
      "Starting epoch 243\n",
      "Loss after mini-batch     1: 0.056\n",
      "Starting epoch 244\n",
      "Loss after mini-batch     1: 0.122\n",
      "Starting epoch 245\n",
      "Loss after mini-batch     1: 0.131\n",
      "Starting epoch 246\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 247\n",
      "Loss after mini-batch     1: 0.027\n",
      "Starting epoch 248\n",
      "Loss after mini-batch     1: 0.058\n",
      "Starting epoch 249\n",
      "Loss after mini-batch     1: 0.079\n",
      "Starting epoch 250\n",
      "Loss after mini-batch     1: 0.072\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "torch_model = DeepNeuralNetwork()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Usaremos 250 epochs para el entrenamiento al igual que TF\n",
    "for epoch in range(0, 250):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    current_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "  \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "        # Perform forward pass\n",
    "        outputs = torch_model(inputs)\n",
    "  \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "  \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "  \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "  \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9af2da-579e-4e62-8d2e-bc191e3a28fb",
   "metadata": {},
   "source": [
    "## Haciendo predicciones con el modelo en PyTorch\n",
    "\n",
    "En este apartado haremos predicciones con el modelo hecho en PyTorch. Primero colocaremos el modelo en modo evaluacion con la funcion `.eval()` que permitirá que el modelo salga del modo de entrenamiento y pase al modo evaluacion. Luego deshabilitaremos el calculo de gradientes mediante `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b1d5732-0f96-4545-b6d2-0bd862c8cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  -2.181509494781494\n"
     ]
    }
   ],
   "source": [
    "# Utilizando el mismo datapoint pre-procesado que utilizamos en tensorflow para la prediccion, crearemos\n",
    "# un tensor de PyTorch.\n",
    "\n",
    "torch_prediction_input = torch.tensor(sample_pre_processed_datapoint.values, dtype=torch.float32)\n",
    "\n",
    "torch_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = torch_model(torch_prediction_input)\n",
    "\n",
    "print(\"Prediction: \", (prediction).float().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68586e63-a242-4311-b866-8b78c70af66d",
   "metadata": {},
   "source": [
    "## Salvando el modelo de PyTorch para servirlo a produccion usando TorchServe\n",
    "\n",
    "Salvamos los pesos (weights) del modelo en un archivo `.pth` que luego sera convertido a un modelo `.mar` el cual es el archivo soportado por TorchServe. Para crear el archivo `.pth` usaremos la funcion `torch.save()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "679fddc9-a9d8-4257-8612-c52cb0f223af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch_model.state_dict(), 'barcamp2024-model-pytorch.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7afb3c-9e97-4a92-b5d6-17e446d82a65",
   "metadata": {},
   "source": [
    "## Convirtiendo modelos a formato ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408e7778-5c8d-40ac-988a-60928c9a8d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742685813.409601 4722971 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1742685813.476855 4722971 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# Tensorflow.\n",
    "tf_spec = [tf.TensorSpec.from_tensor(tensor_preprocessed_datapoint)]\n",
    "tf_output_path = 'barcamp2024-model-tensorflow.onnx'\n",
    "\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=tf_spec, opset=15, output_path=tf_output_path)\n",
    "torch_output_path = 'barcamp2024-model-pytorch.onnx'\n",
    "\n",
    "# PyTorch\n",
    "# nn_inputs=['age', 'gender', 'bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'duration', 'stress_level', 'gender_F', 'gender_M', 'physical_activity_level_Lightly Active', 'physical_activity_level_Sedentary', 'physical_activity_level_Very Active', 'sleep_quality_Excellent', 'sleep_quality_Fair', 'sleep_quality_Good', 'sleep_quality_Poor']\n",
    "nn_inputs=['modelInput']\n",
    "torch.onnx.export(torch_model, torch_prediction_input, torch_output_path, opset_version=13, input_names=nn_inputs, output_names=['modelOutput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b3e19-9ad3-41eb-aa6c-2df10164aeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
