


import numpy as np
import pandas as pd
import sklearn

# Config adicionales para pandas
pd.set_option('display.max_columns', None)

# Importando dataset.
dataset_path = '../Dataset/weight_change_dataset.csv'

dataset_data = pd.read_csv(dataset_path)
dataset_data = dataset_data.drop(['participant_id'], axis=1)
dataset_data.head(10)






# Describiendo el dataset en la parte numerica.
dataset_data.describe()


# Funcionalidad para verificar la distribucion de los datos.
import matplotlib.pyplot as pl
import matplotlib.patches as mpatches

def skewed_distribution(data, features, title):
    figure = pl.figure(figsize = (11,5))
    for i, feature in enumerate(features):
        ax = figure.add_subplot(1, 4, i+1)
        ax.hist(data[feature], bins = 25, color = '#00A0A0')
        ax.set_title("'%s'"%(feature), fontsize = 14)
        ax.set_xlabel("Valor")
        ax.set_ylabel("Numero de registros")
        ax.set_ylim((0, 140))
        ax.set_yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140])
        ax.set_yticklabels([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, ">140"])
        figure.suptitle(title, fontsize = 16, y = 1.03)
        figure.tight_layout()
    
    





features_to_check = ['bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'stress_level']
plot_title = "Distribucion de los features altamente sesgados"
skewed_distribution(dataset_data, features_to_check, plot_title)








# Separacion de la variable objetivo del resto de features.
variacion_peso = dataset_data['weight_change']

# Separacion de los features de la variable objetivo.
features_data = dataset_data.drop('weight_change', axis = 1)





from sklearn.preprocessing import MinMaxScaler

features_sesgados = ['daily_calories_consumed']
features_data[features_sesgados] = dataset_data[features_sesgados].apply(lambda x: np.log(x + 1))
skewed_distribution(features_data, features_to_check, plot_title)


# Normalizacion de los features numericos.

scaler = MinMaxScaler()
features_numericos = ['age', 'current_weight', 'bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'duration', 'stress_level', 'final_weight']
features_data[features_numericos] = scaler.fit_transform(dataset_data[features_numericos])
features_data.head(1)


# Transformacion de features categoricos a numericos (One-Hot Encoding)

features = pd.get_dummies(features_data, dtype='int')
features





import tensorflow as tf
from sklearn.model_selection import train_test_split

def pre_process_data_point(data_point, highly_skewed_features, numerical_features):
    '''
    Pre-process adequately a data point we want to predict
    Args: data_point (dictionary object with the features we want to make a prediction on)
    '''
    data_point_df = pd.DataFrame(data_point, index=[0])
    # data_point_df.reindex(columns=numerical_features)
    print(data_point_df.columns.tolist())
    # print(data_point_df)

    # Aplicando logaritmo a features altamente sesgados.
    data_point_df = data_point_df[highly_skewed_features].apply(lambda x: np.log(x + 1))

    # Escalando los features para que tengan igual tratamiento
    feat_scaler = MinMaxScaler()
    data_point_df[numerical_features] = feat_scaler.fit_transform(data_point_df[numerical_features])

    # Convirtiendo los features categoricos en features numericos.
    data_point_df = pd.get_dummies(data_point_df, dtype='int')
    

    return data_point_df
    

# Split dataset into train, validation and tests sets
def split_dataset(X_data, Y_data, dataset_size, pct_validation, pct_test):

    X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=pct_validation)
    X_train2, X_test, Y_train2, Y_test = train_test_split(X_train, Y_train, test_size=pct_test)

    return X_train2, X_val, X_test, Y_train2, Y_val, Y_test


features_train, features_val, features_test, target_train, target_val, target_test = split_dataset(features, variacion_peso, len(features), 0.15, 0.15)

print(f"Features train length: %s " % len(features_train))
print(f"Features validation length: %s " % len(features_val))
print(f"Features test length: %s " % len(features_test))

# Converting splitted dataset to tensors for the DNN.

t_features_train = tf.convert_to_tensor(features_train)
t_features_val = tf.convert_to_tensor(features_val)
t_features_test = tf.convert_to_tensor(features_test)

t_target_train = tf.convert_to_tensor(target_train)
t_target_val = tf.convert_to_tensor(target_val)
t_target_test = tf.convert_to_tensor(target_test)

# Testing pre-processing function.
sample_data_point = {
    "age": 23,
    "gender": "M",
    "current_weight": 155,
    "bmr": 2500.0,
    "daily_calories_consumed": 3600.0,
    "daily_caloric_surplus_deficit": 50.0,
    "duration": 75.0,
    "stress_level": 70,
    "final_weight": 91
}

data_point_highly_skewed_features = ['daily_calories_consumed']
data_point_numerical_features = ['age', 'current_weight', 'bmr', 'daily_calories_consumed', 'daily_caloric_surplus_deficit', 'duration', 'stress_level', 'final_weight']

print(pre_process_data_point(sample_data_point, data_point_highly_skewed_features, data_point_numerical_features))






# Creando el modelo
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(18,)))
model.add(tf.keras.layers.Dense(15, activation='tanh'))
model.add(tf.keras.layers.Dense(7, activation='tanh'))
model.add(tf.keras.layers.Dense(1, activation='linear'))

model.summary()


# Compilando el modelo
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['r2_score'])


model_epochs = 250
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='weights.best.from_scratch.keras', verbose=1, save_best_only=True)

model.fit(t_features_train, t_target_train, validation_data=(t_features_val, t_target_val), epochs=model_epochs, batch_size=10, callbacks=[checkpoint], verbose=1)



